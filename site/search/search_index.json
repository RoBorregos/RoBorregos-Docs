{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to RoBorregos Documentation","text":"<p>This is the documentation for RoBorregos. Here you can find information about the team, the projects we have worked on, and the tools we use.</p>"},{"location":"#roborregos","title":"RoBorregos","text":"<p>RoBorregos is the Tecnol\u00f3gico de Monterrey's International Robotics Representative Team. We are a team of students passionate about robotics and technology that have several projects in which we participate. To learn more about us, visit our website.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>Projects</li> <li>Util</li> </ul>"},{"location":"#tools","title":"Tools","text":"<ul> <li>ROS</li> </ul>"},{"location":"#development-team","title":"Development team","text":"Name Email Github Role Iv\u00e1n Romero i.wells.ar@gmail.com @IvanRomero03 Software Developer, Repo Mantainer and Automatization Vic @gmail.com @ Software Developer, Repo Mantainer and Automatization Kevin @gmail.com @ PM"},{"location":"RescueMaze/","title":"@RescueMaze","text":"<p>Simulation of a disaster area where the robot has to navigate through the majority of a maze, detect victims through different stimuli (visual images), and evade obstacles. The maze may have multiple floors and the robot must be autonomous.</p>"},{"location":"RescueMaze/#competition","title":"Competition","text":"<p>See the rules for Rescue Maze 2023.</p>"},{"location":"RescueMaze/#sections","title":"Sections","text":"<ul> <li>Jetson Nano</li> </ul>"},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/","title":"Running Project on Jetson nano","text":""},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/#launching-files","title":"Launching files","text":"<ul> <li>Initialize sensors, hector slam, move base, and services:</li> </ul> <pre><code>roslaunch nav_main launch_jetson.launch\n</code></pre> <ul> <li>Run navigation algorithm:</li> </ul> <pre><code>roslaunch exploration main\n</code></pre> <ul> <li>Use rviz from external computer:</li> </ul> <pre><code>export ROS_IP=YOUR_IP\nexport ROS_MASTER_URI=JETSON_IP\n\nrosrun rviz rviz -d $(rospack find robot_description)/rviz/urdf.rviz\n</code></pre>"},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/#connecting-to-jetson-using-ssh","title":"Connecting to Jetson using SSH","text":"<ul> <li>Obtain jetson's IP</li> <li>Perform SSH</li> <li>Introduce password</li> </ul> <pre><code>sudo nmap -sn YOUR_IP/24\nssh username_jetson@JETSON_IP\n</code></pre>"},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/#debug-using-teleop","title":"Debug using teleop","text":"<ul> <li>Make sure ROS_IP and ROS_MASTER_URI are properly set if using another laptop.</li> </ul> <pre><code>rosrun teleop_twist_keyboard teleop_twist_keyboard.py _speed:=0.8 _turn:=2.4 _repeat_rate:=10\n</code></pre>"},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/#add-new-files-to-jetson","title":"Add new files to jetson","text":"<p>If new code was implemented outside of the jetson, the file(s) can be copied using the following command:</p> <pre><code># use -r (recursive flag) for folders\nscp -r SOURCE/ DESTINATION/\n# e.g pass files from laptop to jetson (run command on laptop terminal with ssh connected to jetson)\nscp -r /home/oscar/maze_ws/src/devices/ jetson@IP:/home/jetson/maze_ws/src/\n</code></pre> <p>Also, you may want to consider deleting the files from the jetson first before using scp with the new files:</p> <pre><code># e.g. deleting devices folder before scp\n# in jetson\nrm -rf /home/jetson/maze_ws/src/devices\n</code></pre> <p>Finally, use catkin_make to apply changes.</p> <pre><code>cd ~/maze_ws\ncatkin_make\n</code></pre>"},{"location":"RescueMaze/Jetson%20Nano/USBRules/","title":"USB Port automatization","text":""},{"location":"RescueMaze/Jetson%20Nano/USBRules/#this-file-is-part-of-the-roborregos-rescuemaze-project","title":"This file is part of the RoBorregos RescueMaze project.","text":"<p>Here is an example of how we can se the behaviour of the usb ports.</p> <p></p>"},{"location":"RescueMaze/Jetson%20Nano/USBRules/#udev-rules","title":"Udev Rules","text":"<p>For the USB port automatization, we use udev rules. These rules are located in the <code>rules.d</code> folder. The rules are loaded by the udev daemon when the system starts. The udev daemon monitors the kernel for events and executes the rules when a device is added or removed.</p> <p>To automate the USB ports, we need to create a rule for each port. The rule will be executed when the port is connected to the computer. The rule will execute a script that will set the port to the desired mode.</p> <p>Here is the hard investigation we did to determine the rules:</p> <p></p> <p></p>"},{"location":"home/","title":"@Home","text":"<p>@Home is one of the main competitions for RoBorregos, since it contains a lot of the knowledge that we have acquired throughout the years. It's a complex competitions in multiple levels, which makes it a great challenge for the team.</p>"},{"location":"home/#competition","title":"Competition","text":"<p>The competition consists of a series of tasks that the robot must complete. ...</p>"},{"location":"home/#sections","title":"Sections","text":"<ul> <li>Computer Vision</li> </ul>"},{"location":"home/vision/","title":"Computer Vision","text":"<p>Computer Vision is one of the main areas of development for RoBorregos in the @Home competition. It is a very important area, since it is the one that allows the robot to perceive the environment and interact with it.</p>"},{"location":"home/vision/#sections","title":"Sections","text":"<ul> <li>Object Detection</li> <li>Pose Estimation</li> </ul>"},{"location":"home/vision/pose_estimation/","title":"Pose Estimation with MediaPipe","text":"<p>Pose estimation was implemented using MediaPipe for the RoboCup 2022 @Home Simulation competition. The pose estimation algorithm is based on the MediaPipe Pose solution. </p> <p>It's very simple, acurate and fast. It's also very easy to use, since it's a pre-trained model that can be used directly.</p>"},{"location":"home/vision/pose_estimation/#how-to-use-it","title":"How to use it","text":"<p>First of all, you need to install MediaPipe. You can do it by running the following command:</p> <pre><code>pip install mediapipe\n</code></pre> <p>Then, you can use the following code to get the pose estimation:</p> <pre><code>import mediapipe as mp\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n# Opening the image source to be used\nimage = cv2.imread(\"image.jpg\")\n# Calling the pose detection model\nwith mp_pose.Pose(\nmin_detection_confidence=0.5,\nmin_tracking_confidence=0.5) as pose:\n# Detecting the pose with the image\nposeResult = pose.process(image)\n</code></pre> <p>As a result, you'll have a <code>poseResult</code> array of points. That each point represent a joint of the body, as shown in the following image:</p> <p></p>"},{"location":"home/vision/pose_estimation/#using-pose-estimation-with-webcam","title":"Using pose estimation with webcam","text":"<p>You can also use pose estimation with a webcam to get streamed video. You can use the following code to do it:</p> <pre><code>import mediapipe as mp\nimport cv2\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n# Calling the solution for image drawing from MediaPipe\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n# Opening the webcam\ncap = cv2.VideoCapture(0)\n# Calling the pose detection model\nwith mp_pose.Pose(\nmin_detection_confidence=0.5,\nmin_tracking_confidence=0.5) as pose:\n# Looping through the webcam frames\nwhile cap.isOpened():\n# Reading the webcam frame\nsuccess, image = cap.read()\nif success:\n# Managing the webcam frame\nimage.flags.writeable = False\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n# Detecting the pose with the image\nresults = pose.process(image)\n# Drawing the pose detection results\nimage.flags.writeable = True\nimage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\nmp_drawing.draw_landmarks(\nimage,\nresults.pose_landmarks,\nmp_pose.POSE_CONNECTIONS,\nlandmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\ncv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\nif cv2.waitKey(5) &amp; 0xFF == 27:\nbreak\ncap.release()\n</code></pre> <p>As a result, you'll not only be able to get the pose estimation array. but also the stream with the drawing of the pose estimation.</p> <p>Example:</p> <p></p>"},{"location":"home/vision/pose_estimation/#using-pose-estimation-with-ros","title":"Using pose estimation with ROS","text":"<p>You can receive the image source from a ROS topic. You can use the following code to do it:</p> <pre><code>import mediapipe as mp\nfrom time import sleep\nfrom typing import Tuple\nimport cv2\nimport numpy as np\nimport rospy\nfrom cv_bridge import CvBridge\nfrom sensor_msgs.msg import Image\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n# Calling the solution for image drawing from MediaPipe\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n# Declaring the CvBridge for image conversion from ROS to OpenCV\nbridge = CvBridge()\n# Declaring the image and its callback for the ROS topic\nimageReceved = None\ndef image_callback(data):\nglobal imageReceved\nimageReceved = data\n# Initializing the ROS node\nrospy.init_node('ImageRecever', anonymous=True)\n# Subscribing to the ROS topic\nimageSub = rospy.Subscriber(\n\"/hsrb/head_center_camera/image_raw\", Image, image_callback)\n# Calling the pose detection model\nwith mp_pose.Pose(\nmin_detection_confidence=0.5,\nmin_tracking_confidence=0.5) as pose:\n# Looping through the image frames\nwhile not rospy.is_shutdown():\nif imageReceved is not None:\n# Converting the ROS image to OpenCV\nimage = bridge.imgmsg_to_cv2(imageReceved, \"rgb8\")\n# Detecting the pose with the image\nimage.flags.writeable = False\nresults = pose.process(image)\n# Drawing the pose detection results\nimage.flags.writeable = True\nimage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\nmp_drawing.draw_landmarks(\nimage,\nresults.pose_landmarks,\nmp_pose.POSE_CONNECTIONS,\nlandmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\ncv2.imshow('MediaPipe Pose', image)\nif cv2.waitKey(5) &amp; 0xFF == 27:\nbreak\nelse:\nprint(\"Image not recived\")\nsleep(1)\n</code></pre> <p>Here is an example of the result:</p> <p></p>"},{"location":"soccer/","title":"@SoccerLightweight","text":"<p>Simulation of a disaster area where the robot has to navigate through the majority of a maze, detect victims through different stimuli (visual images), and evade obstacles. The maze may have multiple floors and the robot must be autonomous.</p>"},{"location":"soccer/#competition","title":"Competition","text":"<p>See the rules for Rescue Maze 2023.</p>"},{"location":"soccer/#sections","title":"Sections","text":"<ul> <li>Jetson Nano</li> </ul>"},{"location":"soccer/Electronics/Dribbler%20Implementation/","title":"Dribbler implementation","text":"<p>For the dribbler, a 1000KV brushless motor was used with its respective speed controller that is controlled through a PWM signal from the microcontroller.</p>"},{"location":"soccer/Electronics/General/","title":"General","text":""},{"location":"soccer/Electronics/General/#design-software","title":"Design Software","text":"<p>For the PCB Design the EasyEda software was used. The robot electronics are made up of 5 custom PCBs designed specifically for a specific use. The design of the PCBs was   carried out in the EasyEda software.</p> <p>Initially, the design of the main PCB began. which has measurements of 10 x 10 cm.</p> <p>For the main processing of our robot, an Arduino Mega Pro of 32 Bits and 2 Clocks of 16 MHz was used. It was mainly chosen for its light weight and its size reduction.</p>"},{"location":"soccer/Electronics/General/#drivers","title":"Drivers","text":"<p>HP Pololu 12V Motors at 2200 RPM were used. Our tires are GTF Robots that were in our laboratory from past competitions. For these motors we use Mosfet TB6612FNG type drivers. These drivers are bridged in parallel on the PCB, since HP motors demand a lot of current, especially when forcing between robots, giving us a current continuity of 1.2 and peaks of up to 3 Amps.</p>"},{"location":"soccer/Electronics/General/#how-can-we-handle-this-motors-with-this-drivers","title":"How can we handle this motors with this drivers?","text":"<p>By bridging the motor outputs and input signals, the output channel can be increased up to 2.4 Amps continuous and up to 6 Amps peak. What allows us to use HP motors without danger of having damage to our PCB, drivers, or in the worst case, the motors themselves.</p>"},{"location":"soccer/Electronics/General/#sensors","title":"Sensors","text":"<p>For the control of the robot a BNO 055 gyroscope sensor was used due to its reliability, performance and size. Ultrasonics were used for speed regulation with respect to the detected distance to prevent the robot from getting out of line.</p>"},{"location":"soccer/Electronics/Power%20Supply/","title":"Power Supply","text":"<p>3.7V LIPO batteries were used For the power supply, 3 lipo batteries of 3.7V at 2500 maH were used in a series arrangement, giving them approximately 11.1 v in nominal state.</p> <p>For the logic supply, lipo batteries of 3.7V at 1200 maH were used in a serious arrangement, giving them approximately 7 V in nominal state.</p>"},{"location":"soccer/Electronics/Printed%20Circuit%20Boards%20%28PCB%29/","title":"PCB Designs","text":""},{"location":"soccer/Electronics/Printed%20Circuit%20Boards%20%28PCB%29/#ir-ring","title":"IR Ring","text":"<p>Digital IR receivers: TSSP58038 were used to detect the IR signals emitted by the ball and a custom PCB was also designed.</p> <p>The Ir ring is made up of 12 IR receivers, and an Atmega328p was used for processing and vectoring the infrared signals.</p>"},{"location":"soccer/Electronics/Printed%20Circuit%20Boards%20%28PCB%29/#line-detection-boards","title":"Line Detection Boards","text":"<p>For the detection of the lines, independent PCBs were also designed for each of the three sides in the circumference of the robot.</p> <p>These boards consist of 4 phototransistors each, getting an analog reading by processing our microcontroller on the main board.</p> <p>The phototransistors we used were the TEPT5700. The reason we use these phototransistors is because of their gain, which allows the color reading to have a high difference range. In our case, when we detect white, the phototransistor gives a value close to 300 units. On the other hand, when it detects green, the value decreases to a value of 30, so we have a fairly reliable interval to distinguish between colors.</p> <p>One setback with the choice of phototransistors was the color and incidence of light that had to be thrown at it. Initially the NJL7502L phototransistors were considered, but because these were in a deteriorated state they were discarded. Subsequently, we proceeded to search for those phototransistors that had a peak close to 600 nm, thus preventing them from detecting the infrared signal that is above 700 nm and causing us problems when detecting the lines.</p>"},{"location":"soccer/Electronics/Printed%20Circuit%20Boards%20%28PCB%29/#main-board","title":"Main Board","text":""},{"location":"soccer/Mechanics/More_Characteristics/1.Materials/","title":"Materials","text":"<p>This is the list of mechanical materials we used in the final robot:</p> Name Use Product Image Robot Image ABS filament Printing of most CAD parts of the robot PLA Filament Printing of few CAD parts of the robot Male-female nylon spacers Used for interconnecting the CAD pieces 3x10mm flat-head steel screw Printing of most CAD parts of the robot 3x10mm flat-head steel screw Printing of most CAD parts of the robot"},{"location":"soccer/Mechanics/More_Characteristics/1.Materials/#breakdown-of-the-materials","title":"Breakdown of the materials","text":""},{"location":"soccer/Mechanics/More_Characteristics/1.Materials/#spacers-screws-and-nuts","title":"Spacers, screws and nuts","text":"<p>The materials used to support the robot parts worked very well. The </p>"},{"location":"soccer/Mechanics/More_Characteristics/2.Tools/","title":"Tools","text":""},{"location":"soccer/Mechanics/Robot_Lower_Design/1.Base/","title":"Base","text":""},{"location":"soccer/Mechanics/Robot_Lower_Design/2.Motors/","title":"Motors","text":""},{"location":"soccer/Mechanics/Robot_Lower_Design/3.Wheels/","title":"Wheels","text":""},{"location":"soccer/Mechanics/Robot_Lower_Design/4.Dribbler/","title":"Dribbler","text":""},{"location":"soccer/Mechanics/Robot_Lower_Design/5.Kicker/","title":"Kicker","text":""},{"location":"soccer/Programming/General/","title":"General","text":""},{"location":"soccer/Programming/General/#tools","title":"Tools","text":"<p>The main tools used to program the robot are:</p> <ul> <li>Arduino CLI (Command based interface)</li> <li>Visual Studio Code (IDE)</li> <li>OpenMV IDE (IDE used to program the OpenMV camera)</li> </ul>"},{"location":"soccer/Programming/General/#_1","title":"General","text":""},{"location":"soccer/Programming/General/#strategy","title":"Strategy","text":"<p>During the regional competition we decided to have two attacking robots which had the same code due to time issues and other setbacks. Nonetheless, we learned that this was not a very good strategy, since both robots would sometimes crash with each other when searching for the ball, making scoring very difficult. Therefore, for the national competition we chose the strategy of developing an attacking and a defending robot. Ideally both robots would be able to change roles during the game, however the defending robot had the camera facing backwards, so this was not possible.</p> <p>It is also important to mention that the structure of the code worked as a state machine, advancing to different states until the previous one was completed. This was necessary since it was immportant to keep certain priorities. For example, for the attacking robot, it should first check that it isn't on a line before doing anything else.</p>"},{"location":"soccer/Programming/General/#algorithm","title":"Algorithm","text":""},{"location":"soccer/Programming/General/#attacking-robot","title":"Attacking Robot","text":"<p>The main objective of this robot was to gain possesion of the ball using the dribbler as fast as possible and then go towards the goal using vision. Therefore, the algorithm used is the following:</p> <p></p>"},{"location":"soccer/Programming/General/#defending-robot","title":"Defending Robot","text":"<p>On the other hand, the defending robot should always stay near the goal and go towards it if the ball is in a 20cm radius. The algorithm for this robot is shown in the following image:</p> <p></p>"},{"location":"soccer/Programming/IR_Detection/","title":"IR Detection","text":"<p>For the robot's movements, it was very important to know both the angle and the distance to the ball, so an IR ring was made with 12 IR receivers. However, before calculating these values, it was first necessary to obtain the pulse width of all TPS-58038s, which should be obtained during a ball emission cycle. However, this would imply a time of \\(833 \\mu s \\times  12\\) sensors. Therefore, a better approach was to obtain a reading from all the sensors all at once and repeat this process during the cycle \\((833 \\mu s)\\). Once the method was chosen, the distance and angle had to be calculated. Nonetheless, each had different methods:</p>"},{"location":"soccer/Programming/IR_Detection/#angle","title":"Angle","text":"<p>To obtain the angle towards the ball, there were two main options:</p> <ul> <li>Define the angle as the value of the sensor with the greatest pulse width (12 possible values)</li> <li>Vector calculation (360\u00b0)</li> </ul> <p>Since we wanted more precise values for the angle, we chose to use vector addition to get the resulting vector and hence, its angle. This was possible as each sensor had its unitary vector according to its position in the ring and the magnitud was the pulse width value. Therefore, after getting all sensor values in a cycle, the vectors where added using its x and y components, finally obtaining the angle as the inverse tangent of the result vector.</p>"},{"location":"soccer/Programming/IR_Detection/#distance","title":"Distance","text":"<p>For the distance, there were also different methods:</p> <ul> <li>Obtain the distance from the sensor with the greatest pulse width (its magnitud)</li> <li>Define the distance according to the number of phototransistors that detect the ball (active sensors)</li> <li>Vector calculation (resulting vector magnitud)</li> <li>Define the distance as the sum of the intensity measured by each sensor (combine method 1 and 2)</li> </ul> <p>Even though vector calculation seemed to be the best method, we faced several issues, since it was not very consistent and the result value usually varied within a range from 20 to 40. In addition, the first and second method were also ineficient by themselves, resulting also in a small range of distance. Therefore, both options were combined, which provided the best results, getting a distance with a range from 20 to 100.</p> <p>It is also important to mention, that we used the research of the Yunit team (2017) as a reference. There were also several tests and conclusions done, which can be found in the following document: IR Reasarch.</p>"},{"location":"soccer/Programming/IR_Detection/#detect-possession","title":"Detect Possession","text":"<p>Ideally, we wanted to use the IR Ring to check if the robots had possesion of the ball. However during some tests, we discovered that due to the bright colors, the goals could reflect infrared light emitted by the ball. Therefore, the IR Ring was placed on the robot at a height slightly above the goals to avoid reflections. Nonetheless, this did not allow us to get precise distance measurements when the ball was very close, so we could'nt know if we had possesion or not.  For this reason, another phototransistor was used with the only purpose to determine ball possesion. Similarly to the ring, the sensor counts the readings per cycle to determine the pulse width. However, to reduce noise and get more stable measurements, an Exponential Moving Average (EMA).</p> <pre><code>int detect() {\nint pulseWidth = 0;\nint deltaPulseWidth = 5;\nconst unsigned long startTime_us = micros();\ndo {       filterAnalog.AddValue(analogRead(Constantes::analogo)); //Add value to filter\nif(filterAnalog.GetLowPass() &gt; 700) {                   //Only consider the reading if the \npulseWidth += deltaPulseWidth;                      //infrarred emission is actually significant\n}\n} while((micros() - startTime_us) &lt; 833);\nreturn pulseWidth;\n}\n</code></pre>"},{"location":"soccer/Programming/Movement/","title":"Movement","text":""},{"location":"soccer/Programming/Movement/#holonomic-movemnt","title":"Holonomic movemnt","text":"<p>In order to take advantage of the holonomic drivetrain, the robot had to be able to move in any direction given the desired angle. Therefore, a kinematic model was used in order to determine the speed of each motor. In addition, part of our strategy required our robots to be always facing towards the goal, so the movement had to be corrected if the orientation changed. For this, the BNO-055 was used to get the current orientation and with a simplified PID controller (only P), the error was corrected. The following image shows the kinematic ecuations and corrections implemented:</p> <pre><code>double PID::getError(int target, int cur, int speed) {\nerror = abs(target - cur);\nerror = min(error, 100);  //Limit error to have a max value of 100\nerror *= kP;              //Constant of proportionality\nreturn error;\n}\n</code></pre> <pre><code>void Motors::moveToAngle(int degree, int speed, int error) {\n//Define each speed (values from 0-1)\nfloat m1 = cos(((150 - degree) * PI / 180));\nfloat m2 = cos(((30 - degree) * PI / 180));;\nfloat m3 = cos(((270 - degree) * PI / 180));\n//Multiply by given speed (0-255)\nint speedA = (int(m1 * velocidad));\nint speedB = (int(m2 * velocidad));\nint speedC = (int(m3 * velocidad));\n//Add error\nspeedA += error;\nspeedB += error;\nspeedC += error;\n//Define absolute values\nint abSpeedA = abs(speedA);\nint abSpeedB = abs(speedB);\nint abSpeedC = abs(speedC);\n//Normalize values (to not exceed 255)\nint maxSpeed = max(abSpeedA, max(abSpeedB, abSpeedC));\nif (maxSpeed &gt; 255) {\nabSpeedA = map(abSpeedA, 0, maxSpeed, 0, 255);\nabSpeedB = map(abSpeedB, 0, maxSpeed, 0, 255);\nabSpeedC = map(abSpeedC, 0, maxSpeed, 0, 255);\n}\n//Set speed to each motor\nanalogWrite(motor1.getPwmPin(), abSpeedA);\nanalogWrite(motor2.getPwmPin(), abSpeedB);\nanalogWrite(motor3.getPwmPin(), abSpeedC);\n//Move motors depending on the direction needed\n(speedA &gt;= 0) ? motor1.motorForward() : motor1.motorBackward();\n(speedB &gt;= 0) ? motor2.motorForward() : motor2.motorBackward();\n(speedC &gt;= 0) ? motor3.motorForward() : motor3.motorBackward();\n}\n</code></pre>"},{"location":"soccer/Programming/Movement/#attacking-robot","title":"Attacking robot","text":"<p>In order to take advantage of the HP motors, ideally, the robot should go as fast as possible, however, after a lot of testing, we found that the robot was not able to fully control the ball at high speeds, as it would usually push the ball out of bounds instead of getting it with the dribbler. Therefore, the speed was regulated depending on the distance to the ball (measured with the IR ring) using the following function:</p> <p></p> <p>\\(v(r) = 1.087 + 1/(r-11.5)\\), where \\(r\\) is the distance to the ball \\(\\in [0.00,10.0]\\) </p> <p>This equation was experimentally established with the goal of keeping speed at maximum until the robot gets very close to the ball, when the speed is quickly reduced.</p>"},{"location":"soccer/Programming/Movement/#defending-robot","title":"Defending robot","text":"<p>The idea for this robot was to keep it on the line line of the goal, always looking to keep the ball in front of it to block any goal attempts.Therefore, speed was regulated according to the angle and x-component to the ball. This meant that if the ball was in front of it, then it didn't have to move. However if the ball was far to the right or left, then speed had to be increased proportionally to the x-component of the ball, as shown in the following image:</p> <p></p>"},{"location":"soccer/Programming/Vision/","title":"Vision","text":""},{"location":"soccer/Programming/Vision/#target-detection","title":"Target detection","text":"<p>For goal detections, an Open MV H7 camera was used. Using the Open MV IDE, blob color detection was possible using micropyhton scripts. With this, the bounding box was identified and sent to the arduino. This measures were then used by both robots when going towards the goal or estimating the distance to the goal.</p>"},{"location":"soccer/Programming/Vision/#uart-communication","title":"UART communication","text":"<p>When sending information to the arduino, we faced several issues as the program would sometimes get stuck. We eventually realized that this was due to the protocol that we were using, as the buffer would sometimes receive an incomplete message and get an error when trying to process it. Therefore, to solve this issue, we changed the way to send and receive messages. In python the message was sent in the following format:</p> <pre><code>uart.write(f\"{tag},{x},{y},{w},{h}\\n\")\n</code></pre> <p>Here, the tag value was either an a or a b according to the color of the goal, then the x and y values were the center of the blob and w and h the width and heigth. this message was then received on the arduino on the Serial 3 port, using the following code:</p> <p><pre><code>void updateGoals() {\nif (Serial3.available()) {\nString input1 =  Serial3.readStringUntil('\\n');\nif (input1[0] == 'a')\nyellowGoal.update(input1);\nelse if (input1[0] == 'b')\nblueGoal.update(input1);\n}\n}\n</code></pre> For this first function, we received the bounding box and if the message began with an a, then the object yellow goal was updated and the same thing occured if it began with a b. It was important to first identify when the message began, since sometimes due to the buffer space, messages could be cut and begin with numbers or commas. </p> <p>Then, for the object update, the following code was used:</p> <p><pre><code>void Goal::update(String str) {\nint arr[4];\nString data = \"\";\nint index = 0;\nfor (int i = 2; i &lt; str.length() &amp;&amp; index &lt; 4; i++) {\nif (!(str[i] == ',')) {\ndata += str[i];\n} else if (str[i] == ',' || i == str.length() - 1) {\narr[index++] = data.toInt();\ndata = \"\";\n}\nx = arr[0];\ny = arr[1];\nw = arr[2];\nh = data.toInt();\narea = w * h;\n}\n}\n</code></pre> In this function, it was important for the loop to run until either the length of the string was done or the index got to three, which meant that 4 values were read. It was important to keep this counter, since once again, due to the buffer size, sometimes messages would be cut and combined with other values, which resulted in a longer string with more commas that would eventually cause an error.</p>"},{"location":"util/markdown/","title":"Getting Started with Markdown","text":"<p>Markdown is a simple markup language that allows you to write using a simple sintax. It's used in many places because of how easy it's to use, understand and read.</p>"},{"location":"util/markdown/#headings","title":"Headings","text":"<p>Headings are created using the <code>#</code> symbol. The more <code>#</code> you use, the smaller the heading will be.</p> <p>Example: <pre><code># Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n</code></pre></p>"},{"location":"util/markdown/#text","title":"Text","text":"<p>Text is written as it is. You can use bold and italic text. You can also use ~~strikethrough~~ text. </p> <p>Example: <pre><code>This is a normal text. You can use **bold** and *italic* text. You can also use ~~strikethrough~~ text. \n</code></pre></p>"},{"location":"util/markdown/#lists","title":"Lists","text":"<p>You can create lists using the <code>-</code> symbol. You can also create numbered lists using the <code>1.</code> symbol.</p> <p>Example: <pre><code>- Item 1\n- Item 2\n    - Item 2.1\n    - Item 2.2\n1. Item 1\n2. Item 2\n    1. Item 2.1\n    2. Item 2.2\n</code></pre> Example output:</p> <ul> <li>Item 1</li> <li>Item 2<ul> <li>Item 2.1</li> <li>Item 2.2</li> </ul> </li> <li>Item 1</li> <li>Item 2<ol> <li>Item 2.1</li> <li>Item 2.2</li> </ol> </li> </ul>"},{"location":"util/markdown/#links","title":"Links","text":"<p>You can create links using the <code>[text](link)</code> sintax.</p> <p>Example: <pre><code>[RoBorregos](\n    https://www.roborregos.com\n)\n</code></pre> Example output: RoBorregos</p>"},{"location":"util/markdown/#images","title":"Images","text":"<p>Similar to links, you can add images using the <code>![alt text](image link)</code> sintax.</p> <p>Example: <pre><code>![RoBorregos Logo](https://github.com/RoBorregos.png)\n</code></pre> Example output: </p>"},{"location":"util/markdown/#code","title":"Code","text":"<p>You can add code using the <code>`</code> symbol. You can also add code blocks using the ``` symbol.</p> <p>Example: <pre><code>`print(\"Hello World\")`\n</code></pre> Example output: <code>print(\"Hello World\")</code></p> <p>Example: <pre><code>    ```python\nprint(\"Hello World\")\n    ```\n</code></pre> Example output: <pre><code>print(\"Hello World\")\n</code></pre></p>"},{"location":"util/markdown/#tables","title":"Tables","text":"<p>You can create tables using the <code>|</code> symbol.</p> <p>Example: <pre><code>| Name | Email | Role |\n| ---- | ----- | ---- |\n| Ivan | [i.wells.ar@gmail.com](mailto:i.wells.ar@gmail.com) | Software Developer, Repo Mantainer and Automatization |\n</code></pre></p> <p>Example output:</p> Name Email Role Ivan i.wells.ar@gmail.com Software Developer, Repo Mantainer and Automatization"},{"location":"util/markdown/#quotes","title":"Quotes","text":"<p>You can create quotes using the <code>&gt;</code> symbol.</p> <p>Example: <pre><code>&gt; This is a quote\n</code></pre> Example output:</p> <p>This is a quote</p>"},{"location":"util/markdown/#horizontal-rule","title":"Horizontal Rule","text":"<p>You can create a horizontal rule using the <code>---</code> symbol.</p> <p>Example: <pre><code>---\n</code></pre> Example output:</p>"},{"location":"util/markdown/#todo-list","title":"ToDo List","text":"<p>You can create a task list using the <code>- [ ]</code> symbol.</p> <p>Example: <pre><code>- [ ] ToDo \n- [x] Done ToDo\n</code></pre> Example output:</p> <ul> <li>[ ] ToDo</li> <li>[x] Done ToDo</li> </ul>"}]}