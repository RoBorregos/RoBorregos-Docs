{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to RoBorregos Documentation","text":"<p>This is the documentation for RoBorregos. Here you can find information about the team, the projects we have worked on, and the tools we use.</p>"},{"location":"#roborregos","title":"RoBorregos","text":"<p>RoBorregos is the Tecnol\u00f3gico de Monterrey's International Robotics Representative Team. We are a team of students passionate about robotics and technology that have several projects in which we participate. To learn more about us, visit our website.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>@Home</li> <li>LARC</li> <li>RescueMaze</li> <li>Soccer</li> <li>Util</li> </ul>"},{"location":"#tools","title":"Tools","text":"<ul> <li>ROS</li> </ul>"},{"location":"#development-team","title":"Development team","text":"Name Email Github Role Iv\u00e1n Romero i.wells.ar@gmail.com @IvanRomero03 Repo Mantainer and Documentation Lead"},{"location":"LARC/","title":"LARC 2023","text":""},{"location":"LARC/#latin-american-robotics-competition","title":"Latin American Robotics Competition","text":""},{"location":"LARC/#un-challenge","title":"Un challenge","text":""},{"location":"LARC/2023/","title":"@RescueMaze - 2023","text":"<p>The main developments during 2023 with respect to previous years are the following:</p> <p>TODO: modify this.</p>"},{"location":"LARC/2023/#mechanics","title":"Mechanics","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"LARC/2023/#electronics","title":"Electronics","text":""},{"location":"LARC/2023/#-jetson-nano","title":"- Jetson Nano","text":""},{"location":"LARC/2023/#programming","title":"Programming","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"LARC/2023/Mechanics/Version1/","title":"Version 1 design","text":""},{"location":"LARC/2023/Vision/","title":"Vision","text":"<p>Vision is an area of development on LARC Open challenge that is used to detect different stuff like ArUco codes, letters and colors from different cubes to organize them and interact with them.</p>"},{"location":"LARC/2023/Vision/#sections","title":"Sections","text":"<ul> <li>ArUco Detection</li> <li>Color Detection</li> <li>Letter Classification</li> </ul>"},{"location":"LARC/2023/Vision/ArUco_detection/","title":"ArUco detection","text":""},{"location":"LARC/2023/Vision/ArUco_detection/#aruco","title":"Aruco","text":"<p>An ArUco marker is a synthetic square marker composed of a wide black border and an internal binary matrix that determines its identifier.</p>"},{"location":"LARC/2023/Vision/ArUco_detection/#integration","title":"Integration","text":"<p>ArUco detection was integrated using ArUco library from OpenCV, getting ArUco bounding boxes and aruco class, using the following code:</p> <pre><code>import cv2\nimport cv2.aruco as aruco\nfrom vision_utils import *\nimport numpy as np\n\ndictionary = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n\nparameters = cv2.aruco.DetectorParameters_create()\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    # Capture a frame from the camera\n    ret, frame = cap.read()\n\n    # Convert the frame to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Detect ArUco markers\n    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, dictionary, parameters=parameters)\n\n        # Print list of detected ArUco markers and their corners\n    print(ids)\n\n    if corners:\n        for i, marker_corners in enumerate(corners):\n            print(ids[i])\n            corner = corners[0][0]\n            xmayor = np.amax(corner[:, 0])\n            ymayor = np.amax(corner[:, 1])\n            xmenor = np.amin(corner[:, 0])\n            ymenor = np.amin(corner[:, 1])\n\n            print(f\"Xmayor: {xmayor:.2f}, Xmenor: {xmenor:.2f}, Ymayor: {ymayor:.2f}, Ymenor: {ymenor:.2f}\")    \n    # Draw detected markers on frame\n    frame_with_markers = cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n\n    # Show output frame\n    cv2.imshow(\"Output\", frame_with_markers)\n\n    # Exit if 'q' key is pressed\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\n# Release the camera and close all windows\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p></p>"},{"location":"LARC/2023/Vision/Color_detection/","title":"Color detection","text":""},{"location":"LARC/2023/Vision/Color_detection/#integration","title":"Integration","text":"<p>For detecting colors using OpenCV, we calibrate de color red, blue, green and yellow with HSV color code and then we detect the color of the cube using the a range of values for each color, using the following code:</p> <pre><code>import cv2\nimport numpy as np\n\ndef dibujar(mask,color):\n    contornos,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    for c in contornos:\n        area = cv2.contourArea(c)\n        if area &gt; 3000:\n            M = cv2.moments(c)\n            if (M[\"m00\"]): M[\"m00\"] = 1\n            x = int(M[\"m10\"]/M[\"m00\"])\n            y = int(M['m01']/M[\"m00\"])\n            nuevoContorno = cv2.convexHull(c)\n            #print (nuevoContorno[0][1], nuevoContorno[1], nuevoContorno[2], nuevoContorno[3])\n\n            x, y, w, h = cv2.boundingRect(c)\n            xmin = x\n            ymin = y\n            xmax = x + w\n            ymax = y + h\n            print('xmin:', xmin, 'ymin:', ymin, 'xmax:', xmax, 'ymax:', ymax)\n            print (\"acabeeeeeeeeee\")\n            #cv2.circle(frame,(x,y),7,(0,255,0),-1)  \n            #cv2.putText(frame,'{},{}'.format(x,y), (x+10,y), font, 0.75,(0,255,0),1,cv2.LINE_AA)\n\n            if color == (255,0,0):\n                print('azul')\n            if color == (0,255,0):\n                print('verde')\n            if color == (0,0,255):\n                print('rojo')   \n            if color == (0,255,255):\n                print('Amarillo')\n            cv2.drawContours(frame,[nuevoContorno],0,color,3)\n\n\ncap = cv2.VideoCapture(0)\n\nredBajo1 = np.array([0,150,45],np.uint8)\nredAlto1 = np.array([5,255,200],np.uint8)\n\nredBajo2 = np.array([170,100,45],np.uint8)\nredAlto2 = np.array([179,255,255],np.uint8)\n\nazulBajo = np.array([110,130,45],np.uint8)\nazulAlto = np.array([125,255,255],np.uint8)\n\nverdeBajo = np.array([50,100,20],np.uint8)\nverdeAlto = np.array([80,255,255],np.uint8)\n\namarillobajo = np.array([15,100,20],np.uint8)\namarilloalto = np.array([45,255,255],np.uint8) \n\nfont = cv2.FONT_HERSHEY_SIMPLEX\nwhile True:\n    ret,frame = cap.read()\n    if ret == True:\n        frameHSV = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n        maskAzul = cv2.inRange(frameHSV,azulBajo,azulAlto)\n        maskVerde = cv2.inRange(frameHSV,verdeBajo,verdeAlto)\n        maskamarillo = cv2.inRange(frameHSV,amarillobajo, amarilloalto)\n        maskRed1 = cv2.inRange(frameHSV,redBajo1, redAlto1)\n        maskRed2 = cv2.inRange(frameHSV,redBajo2, redAlto2)\n        maskred = cv2.add(maskRed1,maskRed2)\n        dibujar(maskAzul,(255,0,0))\n        dibujar(maskamarillo,(0,255,255))\n        dibujar(maskVerde,(0,255,0))\n        dibujar(maskred,(0,0,255))\n        frame = cv2.resize(frame, (0, 0), fx = 0.3, fy = 0.3)\n        cv2.imshow('frame',frame)\n        if cv2.waitKey(1) &amp; 0xFF == ord('s'):\n            break\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p> </p>"},{"location":"LARC/2023/Vision/Letter_Clasification/","title":"Letter Clasification","text":""},{"location":"LARC/2023/Vision/Letter_Clasification/#tensorflow-litte","title":"Tensorflow Litte","text":"<p>Tensorflow lite is a framework for running tensorflow models on mobile and embedded devices. It enables on-device machine learning inference with low latency and a small binary size. </p> <p>We used Tensorflow lite because is a easy way to train a funcional model and it don't need a lot of resources to run, so is more efficient than a normal tensorflow model.</p>"},{"location":"LARC/2023/Vision/Letter_Clasification/#installation","title":"Installation","text":"<p>To install Tensorflow Litte you need to run the following command:</p> <pre><code>pip install tflite-model-maker\n</code></pre>"},{"location":"LARC/2023/Vision/Letter_Clasification/#datastet-structure","title":"Datastet structure","text":""},{"location":"LARC/2023/Vision/Letter_Clasification/#usage","title":"Usage","text":""},{"location":"LARC/2023/Vision/Letter_Clasification/#train-a-model","title":"Train a model","text":""},{"location":"LARC/2023/Vision/Letter_Clasification/#import-the-required-packages","title":"Import the required packages","text":"<pre><code>import os\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tflite_model_maker import image_classifier\nfrom tflite_model_maker.image_classifier import DataLoader\n\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"LARC/2023/Vision/Letter_Clasification/#load-the-dataset","title":"Load the dataset","text":"<pre><code>data_set = DataLoader.from_folder(ds_path)\n</code></pre>"},{"location":"LARC/2023/Vision/Letter_Clasification/#split-the-dataset","title":"Split the dataset","text":"<pre><code>train_data, test_data = data_set.split(0.9)\ntest_data, val_data = test_data.split(0.5)\n</code></pre>"},{"location":"LARC/2023/Vision/Letter_Clasification/#train-the-model","title":"Train the model","text":"<pre><code>model = image_classifier.create(train_data)\n</code></pre>"},{"location":"LARC/2023/Vision/Letter_Clasification/#test-the-model","title":"Test the model","text":"<pre><code>loss, accuracy = model.evaluate(test_data)\n</code></pre>"},{"location":"LARC/2023/Vision/Letter_Clasification/#export-the-model","title":"Export the model","text":"<pre><code>model.export(export_dir='.')\n</code></pre>"},{"location":"LARC/2023/Vision/Letter_Clasification/#run-the-model","title":"Run the model","text":"<pre><code>import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\n\ninterpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\ninterpreter.allocate_tensors()\noutput = interpreter.get_output_details()[0]  \n\ninput = interpreter.get_input_details()[0]\ninput_data = tf.constant(1., shape=[1, 1])\n\nimage = \"image.png\"\nimage = cv2.imread(image, cv2.IMREAD_UNCHANGED)\nshape = interpreter.get_input_details()[0]['shape']\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nimage = np.asanyarray(image, dtype=\"uint8\")\nimage = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB) \nimage = cv2.resize(image, (shape[1], shape[2])) \n\nplt.imshow(image)\nimage = image.reshape(shape)\ninterpreter.set_tensor(input['index'], image)\ninterpreter.invoke()\n\nval = (interpreter.get_tensor(output['index'])[0])\nacum = 0\nmax = 0\n\nfor i in range(len(val)):\n    acum += val[i]\n    if val[max] &lt; val[i]:\n        max = i\n\ndata = ['A', 'B', 'C', 'D','E', 'F', ' G', 'H','I']\n\nprint(val)\nprint(data[max])\n</code></pre>"},{"location":"LARC/2024/","title":"@LARC - 2024","text":"<p>The main developments during 2024 with respect to previous years are the following:</p> <p>TODO: modify this.</p>"},{"location":"LARC/2024/#mechanics","title":"Mechanics","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"LARC/2024/#electronics","title":"Electronics","text":""},{"location":"LARC/2024/#-jetson-nano","title":"- Jetson Nano","text":""},{"location":"LARC/2024/#programming","title":"Programming","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"RescueMaze/","title":"@RescueMaze","text":"<p>Simulation of a disaster area where the robot has to navigate through the majority of a maze, detect victims through different stimuli (visual images), and evade obstacles. The maze may have multiple floors and the robot must be autonomous.</p>"},{"location":"RescueMaze/#competition","title":"Competition","text":"<p>See the rules for Rescue Maze 2023.</p>"},{"location":"RescueMaze/#sections","title":"Sections","text":"<ul> <li>Jetson Nano</li> <li>Algorithm</li> <li>ROS</li> <li>Control</li> </ul>"},{"location":"RescueMaze/2023/","title":"@RescueMaze - 2023","text":"<p>The main developments during 2023 with respect to previous years are the following:</p> <p>TODO: modify this.</p>"},{"location":"RescueMaze/2023/#mechanics","title":"Mechanics","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"RescueMaze/2023/#electronics","title":"Electronics","text":""},{"location":"RescueMaze/2023/#-jetson-nano","title":"- Jetson Nano","text":""},{"location":"RescueMaze/2023/#programming","title":"Programming","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"RescueMaze/2023/Algorithm/","title":"Algorithm","text":"<p>The maze solving algorithm is based on some famous algorithms like Breadth First Search (BFS) and Dijkstra's shorthest path  and is coded in C++.</p> <p>The algorithm's main goal is to explore all of the maze in the most efficient way, storing victims and obstacles in a map. And returning to the starting tile at the end. </p>"},{"location":"RescueMaze/2023/Algorithm/#tiles","title":"Tiles","text":"<p>The algorithm's main unit is the tile, where each one has different properties depending on what's physically in there, like black / blue tiles, victims, stairs, ramps, adjacent tiles or walls, which are then used in other processes. There are also properties related to the robot's exploration, like position relative to the robot's starting point in terms of x, y and z, properties defining if a tile has already been visited or if the robot knows if something is in the tile, as well as a weight value that determines the cost to go to that tile.</p>"},{"location":"RescueMaze/2023/Algorithm/#map","title":"Map","text":"<p>The algorithm uses a graph, a data structure that contains different nodes that can have connections to other nodes. In this case, the nodes are the different tiles and the connections are the physical space between them, so a connection exists if the tiles are adjacent to each other. This data structure was chosen for its flexibility, since it can be scaled freely as the robot explores, without having limits on the size of the map. There isn't a specific interface for the graph, instead it is embedded in the tiles, using pointers to have a current tile at all times, as well as a std::map in the tiles that has the pointers to the adjacent tiles in each of the four directions. There's also another std::map at the top level that relates a position to the corresponding tile's pointer, added for a faster lookup. </p> <p></p>"},{"location":"RescueMaze/2023/Algorithm/#engine","title":"Engine","text":"<p>At the start of the execution it assigns the current direction to the north and uses that as a reference for the map. It also initializes relevant variables (map of tiles, first tile, unvisited tiles vector). At every new tile it checks for victims, and stores them if there are, so as to not give additional kits if it passes the victim again.  It then checks the walls on the 4 directions, and if there's no wall it checks if there isn't an existing tile in the adjacent position. If there's an existing tile it appends it to the current tile to avoid duplication, and if there isn't one, it makes a new one and adds it to the vector with the unvisited tiles. This is where the inspiration from the BFS algorithm comes from, since it uses a queue to know the next objectives, but this isn't efficient, since it would do a lot of backtracking. To solve this it uses Dijkstra's algorithm.</p>"},{"location":"RescueMaze/2023/Algorithm/#dijkstras-shortest-path","title":"Dijkstra's Shortest Path","text":"<p>This algorithm helps when trying to find the shortest path to any number of nodes in a graph from a starting node. This specific implementation takes in a starting position in a graph and the list of unvisited tiles. The process of the algorithm involves keeping track of the best path and cost for each tile, as well as a local \"visited\" parameter. It starts in the starting position given to the function and assigns it a cost of 0, and then starts checking the adjacent tiles, comparing the cost to go there from the current position (cost of current tile + weight of adjacent tile) to the current cost value of that tile. If it's lower it updates the cost and path, which would be the current tile. Then it checks for the tile with the lowest cost that has not been visited in the scope of the function. It then repeats the process after all accessible tiles have been visited.</p> <p>The finished product of this loop is the best path and cost for each tile. Also, during the loop there's a variable that keeps track of the best unvisited tile in the scope of the main engine, which is only updated when it's cost is higher than the current one and the current tile is part of the unvisited tiles list. Having the best next tile, it only needs to know the path it needs to take to go there, which is obtained from the path variable, where it uses a grasp of recursion, since the path would be the path to go to the adjacent tile plus a movement to get there. And to actually do that it goes backwards, adding movements to a stack, and then updating the tile to the previous one in the path.</p> <p></p>"},{"location":"RescueMaze/2023/Control/","title":"Control overview","text":"<p>Robot control is the system in charge of moving (and controlling) the robot as expected. This system is of relevance in Rescue Maze as it helps the robot move through obstacles and locate itself, which is fundamental for exploring the whole maze using the algorithm.</p>"},{"location":"RescueMaze/2023/Control/#sensors","title":"Sensors","text":"<p>One of the most important elements needed to develop a stable control is the use of reliable sensors which give feedback to the robot about its current state. The sensor input is then  used to move the robot accordingly and reach the target positions.</p> <p>See Sensors for information about the sensors used.</p>"},{"location":"RescueMaze/2023/Control/#pid","title":"PID","text":"<p>The PID controller is a control loop feedback mechanism widely used in industrial control systems. A PID controller continuously calculates an error value as the difference between a desired setpoint and a measured process variable and applies a correction based on proportional, integral, and derivative terms (sometimes denoted P, I, and D respectively) which give their name to the controller type. </p> <ul> <li>Copilot</li> </ul> <p>In RescueMaze, a PID controller was used to make the robot move straight and rotate to the desired angles. The PID control regulated the PWM signal sent to the motors such that they approached a target RPM. In addition, the RPM targets were increased or diminished depending on the error between the current angle and angle of the desired orientation.</p> <p>See PID for information about the PID implementation.</p>"},{"location":"RescueMaze/2023/Control/PID/","title":"PID","text":"<p>The PID was used to approach target RPMs for the motors. The RPM targets were increased or diminished depending on the error between the current angle and angle of the desired orientation.</p> <p>The PID controller recieved feedback of the motor's speed from the encoders: while rotating, each motor takes a specific amount of pulses to complete a full rotation. The encoders update a counter each time a pulse is detected, and the PID controller uses this information to calculate current RPMs, which determines if the PWM of each specific motor should be increased or decreased to achieve the target. </p> <p>The PID controller has 3 constants: Kp, Ki, and Kd that indicate how much importance should be given to the proportional, integral, and derivative terms, respectively. These constants were tuned manually to achieve the desired performance.</p> <p>See PID implementation here. See examples of calls to the PID object here (notice the calls to attachInterrput, used for encoders). See how to count encoder pulses here.</p>"},{"location":"RescueMaze/2023/Control/PID/#examples-of-tuning-pid-constants","title":"Examples of tuning PID constants","text":"<p>To visualize the effectiveness of the PID controller, the following code was used to plot the RPS of the motors and the target RPS. To use the script:</p> <ol> <li>Set the appropriate COM port in multplePlots.py</li> <li>Close serial monitors from Arduino IDE (otherwise, serial communication will fail)</li> <li>Remove all serial.print() calls from the Arduino code (otherwise, serial communication will fail)</li> <li>From the arduino, call a routine similar to this:</li> </ol> <p></p> <p>Where robot is an instance of Movement and Plot is a class that handles serial communication and plotting. A method to update the motors movement should also be called. In this example, updateStraightPID(RPMs) updates the data and state of the motor, which is sent through serial by the plot class.</p>"},{"location":"RescueMaze/2023/Control/PID/#examples-of-graphs-given-kp-ki-kd-constants","title":"Examples of graphs given kp, ki, kd constants","text":"<p>Example 1</p> <p></p> <p>Example 2</p> <p></p>"},{"location":"RescueMaze/2023/Control/Sensors/","title":"Sensor Overview","text":"<p>The following sensors were used in Rescue Maze 2023:</p> <ul> <li>Time of flight distance sensor: VLX53L0X (Adafruit)</li> <li>RGB Color Sensor: TCS34725 (Adafruit)</li> <li>Absolute Orientation Sensor: BNO055 (Adafruit) </li> </ul>"},{"location":"RescueMaze/2023/Control/Sensors/#using-i2c-devices","title":"Using i2c devices","text":"<p>i2c is a protocol used by many of the sensors used in Rescue Maze. The i2c protocol is a serial communication protocol that allows multiple devices to be connected to the same bus. The advantage of this protocol is that devices only need two wires to communicate (SDA and SCL, in addition to GND and VCC). The disadvantage is that each device needs a unique address to be able to communicate with the master device (in this case, the Arduino).</p> <p>However, since many devices of the same type are used (in the case of Rescue Maze, 4 VLX53L0X distance sensors), then a multiplexor is needed to avoid address conflicts. The multiplexor connects to the i2c bus and allows the  master device to select which device it wants to communicate with. The devices with the same address should be  connected to different multiplexor channels.</p>"},{"location":"RescueMaze/2023/Control/Sensors/#sensor-abstraction","title":"Sensor abstraction","text":"<p>In order to abstract the use of sensors, a class Sensors.h was created. This class is in charge of initializing the majority of the sensors and providing a clean interface to use them. Instead of accessing the sensor data directly, it is best to do so through the sensor class. That way, filters could be applied more easily and if the type of sensor changes, the only thing that needs to be changed is the underlying implementation.</p>"},{"location":"RescueMaze/2023/Control/Sensors/BNO55/","title":"BN055","text":"<p>The BN055 sensor is an absolute orientation sensor that provides useful information to determine the robot's state. In Rescue Maze, it was used to make sure the robot advanced straigth, perform 90-degree turns, and detect when the robot was moving through obstalces or ramps.</p> <p>Product link: https://learn.adafruit.com/adafruit-bno055-absolute-orientation-sensor/overview</p> <p>Library used: https://github.com/adafruit/Adafruit_BNO055</p> <p>The sensor has multiple modes (see Adafruit_BNO055.h lines 61-75). The differences between the modes are the sensors used to calculate the orientation and the output information available. The mode used was:</p> <pre><code>adafruit_bno055_opmode_t mode = OPERATION_MODE_IMUPLUS;\nbno.begin(mode)\n</code></pre>"},{"location":"RescueMaze/2023/Control/Sensors/BNO55/#calibration","title":"Calibration","text":"<p>The sensor needs to be calibrated before it can be used. The calibration process is explained here. The sensor must be moved in specific directions to calibrate each sensor. The accelerometer and gyroscope can be calibrated using preloded offsets, but the magnetometer needs each time the sensor powers off. To obtain the offsets for calibration, the following code was used. Once the sensor is calibrated, the program prints the offset values to serial. An example of how to load these values to the BNO055 can be found here (See BNO::restoreCalibration).</p>"},{"location":"RescueMaze/2023/Control/Sensors/TCS34725/","title":"TCS34725","text":"<p>The TCS34725 is a color sensor that outputs the detected color in RGB format. In Rescue Maze, it was used to identify blue and black tiles. The sensor was placed in the front of the robot to be able to make detections before more than half of the robot entered the tile.</p> <p>Product link: https://www.adafruit.com/product/1334</p> <p>Library used: https://github.com/adafruit/Adafruit_TCS34725</p> <p>The sensor has multiple modes (see Adafruit_TCS34725.h lines 145-181). Each mode changes the time that the sensor takes to make a detection, the tradeoff being that the more time it takes, the more accurate the detection is. The mode used in the rescue maze was:</p> <pre><code>Adafruit_TCS34725 tcs = Adafruit_TCS34725(TCS34725_INTEGRATIONTIME_50MS, TCS34725_GAIN_1X);\n</code></pre> <p>The main methods we used to interact with the Adafruit_TCS34725 object are:</p> <pre><code>tcs.getRawData(&amp;red_r, &amp;green_r, &amp;blue_r, &amp;clear_r);\ntcs.getRGB(&amp;red, &amp;green, &amp;blue);\n</code></pre> <p>When comparing the rgb output of both methods, getRawData displayed a greater difference between colors and was thus used.</p>"},{"location":"RescueMaze/2023/Control/Sensors/TCS34725/#processing-rgb-input","title":"Processing RGB input","text":"<p>The idea behind our implementation to relate a RGB output with a color was very simple:</p> <ul> <li>For each color, establish minimum and maximum expected values for each color channel.</li> </ul> <pre><code>static constexpr char colorList[colorAmount + 1] = {\"NAG\"}; // List of color initials\n// Each row represents the upper and lower limits for detecting a color.\n// colorThresholds[0] = {redMin, redmax, greenMin, greenMax, blueMin, blueMax}\nstatic constexpr int colorThresholds[colorAmount][6] = {\n{0, 40, 0, 40, 0, 40},\n{60, 100, 120, 150, 125, 175},\n{180, 240, 190, 210, 180, 210}};\n</code></pre> <ul> <li> <p>After each reading, check if the values are within the expected range for each color.</p> </li> <li> <p>Return a value indicating a specific color if all three channels match the expected range.</p> </li> </ul> <pre><code>char TCS::getColorWithThresholds()\n{\n  if (colorThresholds == nullptr)\n    return 'u';\n\n  updateRGBC(); // Update the RGB values\n\n  for (uint8_t i = 0; i &lt; colorAmount; i++) {\n    if (inRangeThreshold(colorThresholds[i][0], red, colorThresholds[i][1]) &amp;&amp; inRangeThreshold(colorThresholds[i][2], green, colorThresholds[i][3]) &amp;&amp; inRangeThreshold(colorThresholds[i][4], blue, colorThresholds[i][5])){\n      return colorList[i];\n    }\n  }\n\n  return 'u'; // In case no color is detected.\n}\n</code></pre>"},{"location":"RescueMaze/2023/Control/Sensors/VLX53L0X/","title":"VLX53L0X","text":"<p>The VLX53L0X is a time of flight distance sensor. In Rescue Maze, it was used to detect walls and robot displacement.</p> <p>Product link: https://www.adafruit.com/product/3317</p> <p>Library used: https://github.com/adafruit/Adafruit_VL53L0X</p> <p>The sensor has multiple modes (see Adafruit_VL53L0X.cpp lines 210-219). </p>"},{"location":"RescueMaze/2023/Control/Sensors/VLX53L0X/#considerations","title":"Considerations","text":"<p>While this sensor gives accurate readings, it is not very reliable for long distances. Specifically, its maximum range is ~1.3 meters and its acurracy decreases the further the object is. In some scenarios, the sensor wasn't reliable to measure robot displacement.</p>"},{"location":"RescueMaze/2023/Jetson%20Nano/RunningJetson/","title":"Running Project on Jetson nano","text":""},{"location":"RescueMaze/2023/Jetson%20Nano/RunningJetson/#launching-files","title":"Launching files","text":"<ul> <li>Initialize sensors, hector slam, move base, and services:</li> </ul> <pre><code>roslaunch nav_main launch_jetson.launch\n</code></pre> <ul> <li>Run navigation algorithm:</li> </ul> <pre><code>roslaunch exploration main\n</code></pre> <ul> <li>Use rviz from external computer:</li> </ul> <pre><code>export ROS_IP=YOUR_IP\nexport ROS_MASTER_URI=JETSON_IP\n\nrosrun rviz rviz -d $(rospack find robot_description)/rviz/urdf.rviz\n</code></pre>"},{"location":"RescueMaze/2023/Jetson%20Nano/RunningJetson/#connecting-to-jetson-using-ssh","title":"Connecting to Jetson using SSH","text":"<ul> <li>Obtain jetson's IP</li> <li>Perform SSH</li> <li>Introduce password</li> </ul> <pre><code>sudo nmap -sn YOUR_IP/24\nssh username_jetson@JETSON_IP\n</code></pre>"},{"location":"RescueMaze/2023/Jetson%20Nano/RunningJetson/#debug-using-teleop","title":"Debug using teleop","text":"<ul> <li>Make sure ROS_IP and ROS_MASTER_URI are properly set if using another laptop.</li> </ul> <pre><code>rosrun teleop_twist_keyboard teleop_twist_keyboard.py _speed:=0.8 _turn:=2.4 _repeat_rate:=10\n</code></pre>"},{"location":"RescueMaze/2023/Jetson%20Nano/RunningJetson/#add-new-files-to-jetson","title":"Add new files to jetson","text":"<p>If new code was implemented outside of the jetson, the file(s) can be copied using the following command:</p> <pre><code># use -r (recursive flag) for folders\nscp -r SOURCE/ DESTINATION/\n# e.g pass files from laptop to jetson (run command on laptop terminal with ssh connected to jetson)\nscp -r /home/oscar/maze_ws/src/devices/ jetson@IP:/home/jetson/maze_ws/src/\n</code></pre> <p>Also, you may want to consider deleting the files from the jetson first before using scp with the new files:</p> <pre><code># e.g. deleting devices folder before scp\n# in jetson\nrm -rf /home/jetson/maze_ws/src/devices\n</code></pre> <p>Finally, use catkin_make to apply changes.</p> <pre><code>cd ~/maze_ws\ncatkin_make\n</code></pre>"},{"location":"RescueMaze/2023/Jetson%20Nano/USBRules/","title":"USB Port automatization","text":""},{"location":"RescueMaze/2023/Jetson%20Nano/USBRules/#this-file-is-part-of-the-roborregos-rescuemaze-project","title":"This file is part of the RoBorregos RescueMaze project.","text":"<p>Here is an example of how we can se the behaviour of the usb ports.</p> <p></p>"},{"location":"RescueMaze/2023/Jetson%20Nano/USBRules/#udev-rules","title":"Udev Rules","text":"<p>For the USB port automatization, we use udev rules. These rules are located in the <code>rules.d</code> folder. The rules are loaded by the udev daemon when the system starts. The udev daemon monitors the kernel for events and executes the rules when a device is added or removed.</p> <p>To automate the USB ports, we need to create a rule for each port. The rule will be executed when the port is connected to the computer. The rule will execute a script that will set the port to the desired mode.</p> <p>Here is the hard investigation we did to determine the rules:</p> <p></p> <p></p>"},{"location":"RescueMaze/2023/ROS/","title":"Robot Operating System","text":"<p>The usage of Robot Operating System was crucial for features like vision, navigation as well as the integration of all the systems as a whole.</p> <ul> <li>Navigation</li> <li>Serial Communication</li> </ul>"},{"location":"RescueMaze/2023/ROS/Navigation/","title":"Navigation","text":"<p>A navigation system was developed for a more flexible robot in terms of adaptability, since using a real map can help get more precision compared to a system that uses unitary movements.</p> <p>The implementation was done using the navigation stack provided by ROS. To get the setup running there are some requirements that need to be met, such as the sensor sources, the transform configuration, the odometry information, the base controller, the map, the robot model, and the navigation stack itself.</p>"},{"location":"RescueMaze/2023/ROS/Navigation/#sensor-sources","title":"Sensor sources","text":"<p>The sensor sources are the sensors that the robot uses to get information about its surroundings. In this case, the robot uses a 2D LiDAR. In this case, a LD06 LiDAR was used initially, but due to the noise in the readings, it was changed to a YDLIDAR Tmini Pro. To use the LiDAR, each one had its own ROS package, which was used to get the information from the sensor and publish it to a topic. The packages used were:</p> <ul> <li>ld06</li> <li>ydlidar</li> </ul> <p>Each package has its own launch file, which is used to launch the node that publishes the information to the topic <code>/scan</code>. The launch files used were:</p> <ul> <li>ld06.launch</li> <li>Tmini.launch</li> </ul> <p>Another sensor used was the IMU, which was used to get the orientation of the robot. The IMU used was the BNO055, which wass used with the package bno055. The launch file used to launch the node that publishes the information to the topic <code>/imu/data</code> was: </p> <ul> <li>imu.launch</li> </ul>"},{"location":"RescueMaze/2023/ROS/Navigation/#map","title":"Map","text":"<p>The map was created using the Hector SLAM package, which is a SLAM algorithm that can be used without odometry. The map was created using a 2D LiDAR. To launch the mapping process, the following command was used:</p> <pre><code>roslaunch nav_main hs_mapping.launch\n</code></pre> <p>This command launches the mapping process and the RViz visualization. The mapping process is done by moving the robot around the environment, so that the LiDAR can get information about the environment. </p>"},{"location":"RescueMaze/2023/ROS/Navigation/#planning","title":"Planning","text":"<p>The planning is done using the navigation stack provided by ROS. Once setup, the navigation stack receives information from different sources, as well as different goals from the algorithm. Upon receiving a goal, the navigation stack plans a path to the goal and calculates the velocity commands to send to the robot. In this case, the planner used was the dwa_local_planner, which is a local planner that uses the Dynamic Window Approach to calculate the velocity commands. The velocity commands are calculated using the information from the sensor sources, the odometry information, the map, and the robot model. </p> <p>The velocity commands were initially published to the <code>/cmd_vel</code> topic, which is used by the base controller to move the robot. However, in order to utilize the limit switches, the velocity commands were published to another topic, and another node was created to listen to the velocity commands from the planner, as well as the velocity commands published by the algorithm (recovery situations), and published to the <code>/cmd_vel</code> topic prioritizing the recovery situations. The node used to do this was the mux_cmd_vel custom node.</p>"},{"location":"RescueMaze/2023/ROS/Navigation/#sending-goals","title":"Sending goals","text":"<p>The goals are sent to the navigation stack using the move_base package. In this case the goals were limited to the goals sent by the algorithm, being only 90 degree turns and 30 cm movements forward or backward. In order to send accurate goals, a custom transform was used, which is used to represent the ideal position of the robot at any given moment, compensating for incaccuracies in the robot's translational and rotational movement. </p> <p>This transform was calculated by using the IMU yaw data as well as the localization_grid data and was published by a transform broadcaster.</p> <ul> <li>IMU data: Stored when the robot is initialized, and used to calculate the angle of each cardinal direction, which are then used to update the transform.</li> <li>Localization grid: Used to get the distance from the robot to the center of the current tile. Used to update the transform to send goals from the center of the tile.</li> </ul>"},{"location":"RescueMaze/2023/ROS/Navigation/#problems","title":"Problems","text":"<p>Multiple problems were encountered during the development of the navigation system. The main problem was the noise in the LiDAR readings, which caused the robot to generate wrong maps, making it impossible to navigate. This led to trying different LiDARs, as well as implementing filters for the LiDAR readings. The filters used included a custom script that reduced the laser scan readings by a given factor, allowing a faster processing and a consistent number of points per scan, as well as the laser_filters package, which was used to filter the laser scan readings that were too close to the robot, as well as the readings that were too far from the robot.</p>"},{"location":"RescueMaze/2023/ROS/SerialCommunication/","title":"Serial Communication","text":"<p>Serial communication is used to communicate with the Arduino. The Arduino is connected to the Jetson Nano via USB. The communication is handled as a master-slave relationship, where the Jetson Nano is the master and the Arduino is the slave. The Arduino is programmed to listen to the Jetson Nano and respond accordingly. </p>"},{"location":"RescueMaze/2023/ROS/SerialCommunication/#initial-communication-implementation","title":"Initial Communication Implementation","text":"<p>Initially, the communication was implemented using the rosserial library. The library is used to create a node that is used to send and receive data from the Arduino, allowing the use of ROS topics and services. However, due to problems with the library, the implementation was changed to use the python serial library.</p>"},{"location":"RescueMaze/2023/ROS/SerialCommunication/#jetson-nano-implementation","title":"Jetson Nano Implementation","text":"<p>The Jetson Nano uses the serial library to communicate with the Arduino. The library is used to create a serial object that is used to send and receive data. </p> <p>The code implementation uses a set of functions that use specific commands to communicate with the Arduino. Each function uses a specific action number and a number of bytes to expect from the Jetson Nano. The Arduino is programmed to listen to the action number and respond accordingly. These functions include sending and receiving sensor data, as well as sending commands to the Arduino.  This implementation also has a specific process to handle the sending and receiving of data, adding overhead to the packets sent and received. </p>"},{"location":"RescueMaze/2023/Vision/","title":"Vision","text":"<p>Computer vision is an important area in Rescue Maze as it allows to complete some of the tasks that provide the most points. In Maze, vision was used to detect visual and colored victims, which give feedback to the robot of when should rescue kits be dropped.</p> <p>The cameras used in this competition were connected to the Jetson Nano to process the output and perform the necessary tasks. In particular, 2 openmv cameras were used, one for each side of the robot. The cameras were connected to the Jetson Nano via USB and used Serial communication.</p> <p>Both cameras runned a RPC script, which allowed the Jetson Nano to send commands to the cameras and receive data from them. 2 main calls were performed: one to get the image from the camera (to process in Jetson) and another to get the main detected color (processed in the camera).</p>"},{"location":"RescueMaze/2023/Vision/#sections","title":"Sections","text":"<ul> <li>OpenMV: More information of how to use the OpenMVs and RPC.</li> <li>TensorFlow Lite: More information of how to train models and use them for image inference.</li> </ul>"},{"location":"RescueMaze/2023/Vision/Openmv/","title":"OpenMV","text":"<p>OpenMV is a project that aims to provide low-cost and easy-to-use machine vision modules. The OpenMV cameras run on MicroPython (Python 3), and can be programmed using the OpenMV IDE that facilitates development and contains additional tools.</p> <ul> <li>Camera used: Openmv H7 R2</li> <li>Documentation: Openmv Documentation</li> </ul>"},{"location":"RescueMaze/2023/Vision/Openmv/#rpc","title":"RPC","text":"<p>The rpc module on the OpenMV Cam allows you to connect your OpenMV Cam to another microcontroller or computer and execute remote python (or procedure) calls on your OpenMV Cam. The rpc module also allows for the reverse too if you want your OpenMV Cam to be able to execute remote procedure (or python) calls on another microcontroller or computer.</p> <p>-Openmv Docs</p> <p>See the script used in the OpenMVs to detect colors and send images through RPC.</p> <p>See the code used to make calls to OpenMVs and interpret camera results here.</p>"},{"location":"RescueMaze/2023/Vision/TensorFlowLite/","title":"TensorFlow Lite","text":"<p>TensorFlow Lite is a framework that allows to run machine learning models on embedded devices. It is a lightweight version of TensorFlow, which is a framework for machine learning developed by Google. TensorFlow Lite is used in Rescue Maze to run machine learning models on the Jetson Nano, which is the embedded device used in the robot.</p> <p>- Copilot</p> <p>There are 3 main steps to use TensorFlow Lite in Rescue Maze:</p> <ol> <li>Generate the dataset</li> <li>Train a model</li> <li>Run the model on the Jetson Nano</li> </ol>"},{"location":"RescueMaze/2023/Vision/TensorFlowLite/#generate-the-dataset","title":"Generate the dataset","text":"<p>See a dataset example here. The dataset is composed of images taken with the OpenMV camera. The images are divided in folders, each corresponding to a specific class. The images of each class should take into account the varitions of how the object can be seen in the camera. For example, if the object is a victim, the images should be taken from different angles, distances, and lighting conditions. The scripts used to generate the datasets were: dataset_capture_script.py (runned in the OpenMV), and save_image.py (runned in external computer, where images were saved).</p>"},{"location":"RescueMaze/2023/Vision/TensorFlowLite/#train-a-model","title":"Train a model","text":"<ul> <li>Here is an end-to-end example of process to train a new model.</li> <li>Here is the script used to train this year's models: image_classifier_maze.ipynb</li> </ul>"},{"location":"RescueMaze/2023/Vision/TensorFlowLite/#run-the-model-on-the-jetson-nano","title":"Run the model on the Jetson Nano","text":"<ul> <li>Here is the script used to run the models on the Jetson Nano: camera_controller.py</li> </ul> <p>Once the model is trained, the script will ouput a .tflite file. This file is loaded in camera_controller.py to make inferences. The model returns an array of probabilities, each corresponding to a class.</p>"},{"location":"RescueMaze/2024/","title":"@RescueMaze - 2024","text":"<p>The main developments during 2024 with respect to previous years are the following:</p> <p>TODO: modify this.</p>"},{"location":"RescueMaze/2024/#mechanics","title":"Mechanics","text":"<ul> <li>Jetson Nano</li> </ul>"},{"location":"RescueMaze/2024/#electronics","title":"Electronics","text":"<ul> <li>Jetson Nano</li> </ul>"},{"location":"RescueMaze/2024/#programming","title":"Programming","text":"<ul> <li>Jetson Nano</li> </ul>"},{"location":"SoccerLightweight/","title":"@SoccerLightweight","text":"<p>2 vs 2 autonomous robot competition where opposing team robots have to play soccer playoffs. The twist of this competition is that robots have to weigh less than 1.1 kg, hence the name \"Soccer Lightweight\".</p>"},{"location":"SoccerLightweight/#competition","title":"Competition","text":"<p>See the rules for Soccer Lightweight 2023.</p>"},{"location":"SoccerLightweight/#sections","title":"Sections","text":""},{"location":"SoccerLightweight/#mechanics","title":"Mechanics","text":"<ul> <li> <p>General</p> </li> <li> <p>Robot Lower Design</p> </li> <li> <p>Robot Upper Design</p> </li> </ul>"},{"location":"SoccerLightweight/#electronics","title":"Electronics","text":"<ul> <li> <p>General</p> </li> <li> <p>Power Supply</p> </li> <li> <p>Printed Circuit Boards (PCB)</p> </li> <li> <p>Dribbler Implementation</p> </li> </ul>"},{"location":"SoccerLightweight/#programming","title":"Programming","text":"<ul> <li> <p>General</p> </li> <li> <p>IR Detection</p> </li> <li> <p>Line Detection</p> </li> <li> <p>Movement</p> </li> <li> <p>Vision</p> </li> </ul>"},{"location":"SoccerLightweight/2023/","title":"@SoccerLightweight - 2023","text":"<p>The main developments during 2023 with respect to previous years are the following:</p> <p>TODO: modify this.</p>"},{"location":"SoccerLightweight/2023/#mechanics","title":"Mechanics","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"SoccerLightweight/2023/#electronics","title":"Electronics","text":""},{"location":"SoccerLightweight/2023/#-jetson-nano","title":"- Jetson Nano","text":""},{"location":"SoccerLightweight/2023/#programming","title":"Programming","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"SoccerLightweight/2023/Electronics/1%20General/","title":"General","text":""},{"location":"SoccerLightweight/2023/Electronics/1%20General/#design-software","title":"Design Software","text":"<p>For the PCB Design the EasyEda software was used.</p> <p>The robot electronics are made up of 5 custom PCBs designed specifically for a specific use. The design of the PCBs was   carried out in the EasyEda software.</p>"},{"location":"SoccerLightweight/2023/Electronics/1%20General/#drivers","title":"Drivers","text":"<p>HP Pololu 12V Motors at 2200 RPM were used. Our tires are GTF Robots that were in our laboratory from past competitions. For these motors we use Mosfet TB6612FNG type drivers. These drivers are bridged in parallel on the PCB, since HP motors demand a lot of current, especially when forcing between robots, giving us a current continuity of 1.2 and peaks of up to 3 Amps.</p>"},{"location":"SoccerLightweight/2023/Electronics/1%20General/#how-can-we-handle-this-motors-with-this-drivers","title":"How can we handle this motors with this drivers?","text":"<p>By bridging the motor outputs and input signals, the output channel can be increased up to 2.4 Amps continuous and up to 6 Amps peak. What allows us to use HP motors without danger of having damage to our PCB, drivers, or in the worst case, the motors themselves.</p>"},{"location":"SoccerLightweight/2023/Electronics/1%20General/#sensors","title":"Sensors","text":"<p>For the control of the robot a BNO 055 gyroscope sensor was used due to its reliability, performance and size. Ultrasonics were used for speed regulation with respect to the detected distance to prevent the robot from getting out of line.</p>"},{"location":"SoccerLightweight/2023/Electronics/2%20Power%20Supply/","title":"Power Supply","text":"<p>3.7V LIPO batteries were used For the power supply, 3 lipo batteries of 3.7V at 2500 maH were used in a series arrangement, giving them approximately 11.1 v in nominal state.</p> <p>For the logic supply, lipo batteries of 3.7V at 1200 maH were used in a serious arrangement, giving them approximately 7 V in nominal state.</p> <p>The batteries played an important role in the elaboration of our robot because the weight of the battery is relatively heavy compared to other components such as the driver or the Arduino Board itself.</p>"},{"location":"SoccerLightweight/2023/Electronics/2%20Power%20Supply/#why-did-we-use-lipo-batteries","title":"Why did we use LiPo Batteries?","text":""},{"location":"SoccerLightweight/2023/Electronics/2%20Power%20Supply/#high-energy-density","title":"High energy density","text":"<p>LiPo batteries offer a higher energy density compared to traditional battery technologies like nickel-cadmium (NiCd) or nickel-metal hydride (NiMH). This means they can store more energy per unit volume or weight, providing longer operating times and increased power for portable devices.</p>"},{"location":"SoccerLightweight/2023/Electronics/2%20Power%20Supply/#lightweight-and-compact","title":"Lightweight and compact","text":"<p>LiPo batteries are incredibly lightweight and have a slim profile, making them ideal for applications where space and weight are critical factors. They are commonly used in drones, RC vehicles, smartphones, and wearable devices, allowing for more compact designs and improved portability.</p>"},{"location":"SoccerLightweight/2023/Electronics/2%20Power%20Supply/#high-discharges-rates","title":"High discharges rates","text":"<p>LiPo batteries can deliver high discharge rates, making them suitable for applications that require bursts of high power. This characteristic is particularly advantageous in robotics, electric vehicles, and high-performance hobbyist applications.</p>"},{"location":"SoccerLightweight/2023/Electronics/2%20Power%20Supply/#fast-charging","title":"Fast charging","text":"<p>LiPo batteries can be charged at a much faster rate compared to other battery chemistries. They can accept higher charge currents, reducing charging times and improving overall device usability.</p>"},{"location":"SoccerLightweight/2023/Electronics/3%20Printed%20Circuit%20Boards%20%28PCB%29/","title":"PCB Designs","text":""},{"location":"SoccerLightweight/2023/Electronics/3%20Printed%20Circuit%20Boards%20%28PCB%29/#ir-ring","title":"IR Ring","text":"<p>Digital IR receivers: TSSP58038 were used to detect the IR signals emitted by the ball and a custom PCB was also designed.</p> <p>The Ir ring is made up of 12 IR receivers, and an Atmega328p was used for processing and vectoring the infrared signals.</p>"},{"location":"SoccerLightweight/2023/Electronics/3%20Printed%20Circuit%20Boards%20%28PCB%29/#line-detection-boards","title":"Line Detection Boards","text":"<p>For the detection of the lines, independent PCBs were also designed for each of the three sides in the circumference of the robot.</p> <p>These boards consist of 4 phototransistors each, getting an analog reading by processing our microcontroller on the main board.</p> <p>The phototransistors we used were the TEPT5700. The reason we use these phototransistors is because of their gain, which allows the color reading to have a high difference range. In our case, when we detect white, the phototransistor gives a value close to 300 units. On the other hand, when it detects green, the value decreases to a value of 30, so we have a fairly reliable interval to distinguish between colors.</p> <p>One setback with the choice of phototransistors was the color and incidence of light that had to be thrown at it. Initially the NJL7502L phototransistors were considered, but because these were in a deteriorated state they were discarded. Subsequently, we proceeded to search for those phototransistors that had a peak close to 600 nm, thus preventing them from detecting the infrared signal that is above 700 nm and causing us problems when detecting the lines.</p>"},{"location":"SoccerLightweight/2023/Electronics/3%20Printed%20Circuit%20Boards%20%28PCB%29/#main-board","title":"Main Board","text":"<p>The main board consists basically of an Arduino Mega Pro Shield for controlling the dedicated actuators and sensors for the robot to operate correctly.</p> <p>Initially, the design of the main PCB began. which has measurements of 10 x 10 cm.</p> <p>For the main processing of our robot, an Arduino Mega Pro of 32 Bits and 2 Clocks of 16 MHz was used. It was mainly chosen for its light weight and its size reduction.</p>"},{"location":"SoccerLightweight/2023/Electronics/4%20Dribbler%20Implementation/","title":"Dribbler implementation","text":"<p>For the dribbler, a 1000KV brushless motor was used with its respective speed controller that is controlled through a PWM signal from the microcontroller.</p>"},{"location":"SoccerLightweight/2023/Mechanics/0.General/","title":"General","text":""},{"location":"SoccerLightweight/2023/Mechanics/0.General/#materials","title":"Materials","text":"<p>This is the list of mechanical materials we used while developing the robot:</p> Name Use Product Image 3mm MDF Made up earlier parts of the robot ABS filament Make up most CAD parts of the robot PLA filament Make up few CAD parts of the robot Male-female nylon spacers Connecting separated robot pieces M3 6mm round-head nylon screws Fixing in place the robot pieces M3 10mm flat-head steel screws Fixing in place the robot pieces M3 nylon nuts Fixing in place the robot pieces M3 steel nuts Fixing in place the robot pieces <ul> <li>Piece materials</li> </ul> <p>We did an extensive research on observing what material could be used that is both lightweight and impact resistant. The material we first used was MDF, because it was cheap, easy to manufacture, and lightweight, but it wasn't very impact resistant. Other teams where using carbon fiber fillaments or carbon fiber plates, but these were out of our budget. We used PLA to make parts that we couldn't make on MDF, but this material is more heavy and also not impact resistant. </p> <p>We ended up settling on ABS, which is a weight intermediate between MDF and PLA, but is very impact resistant. This material, however, is very difficult to print. The results after printing definetely showed a considerable impact improvement over the MDF used earlier, with only a slight increase on weight. </p> <p>The reason we didn't make the robot entirely out of ABS was mostly for time and the difficulty of printing it, but because it's less dense than PLA it would've definetely been better if we made it all out of ABS. Not to mention it would've been more aesthetically pleasing to have it all black.</p> <ul> <li>Support materials</li> </ul> <p>The materials used to support the robot worked very well. The combination of nylon spacers, metal nuts and screws made it so that we had a sturdy and lightweight support for the pieces. And if the support didn't need to be as sturdy, we used nylon screws and nuts. We also found that the best way to place the spacers is upside down: putting the screw at the bottom and the nut at the top. This is because gravity paired with the vibrations of movement and crashes makes it so that the nuts fall out easier.</p>"},{"location":"SoccerLightweight/2023/Mechanics/0.General/#tools","title":"Tools","text":"<p>This is the list of tools we used to manufacture the robot:</p> Name Use Product Image Fusion360 CAD program Laser cutter Cutting MDF Ender 3 V2 Printing PLA Artillery Sidewinder X2 Printing ABS <ul> <li>Software</li> </ul> <p>I used Fusion360 for the robot's design because it's the one i'm more used to and because it's compatible with my operating system. Our team uses Solidworks a lot more tho, so although I think Fusion360 works very well it's better to all use the same program so that other members can help you easier.</p> <ul> <li>Hardware</li> </ul> <p>The machines we used where either ours or from the laboratories of our institution. The laser cutter and the Sidewinder were located in the SmartFactory laboratr, while the the Ender is in our laboratory. </p> <p>As stated before, ABS was very difficult to print. We tried using our Ender, but the resulting pieces didn't look right and it also caused some problems when the nozzle plugged and caused the hot ABS to melt the plastic cover of the printer. We tried using to</p> <p>We would definetely recommend using the Zortrax material so that the </p>"},{"location":"SoccerLightweight/2023/Mechanics/0.General/#robot-timeline","title":"Robot timeline","text":"<p>These are the robot's different versions as we progressed in its design:</p> <p></p>"},{"location":"SoccerLightweight/2023/Mechanics/1.Robot_Lower_Design/","title":"Lower Design","text":""},{"location":"SoccerLightweight/2023/Mechanics/1.Robot_Lower_Design/#base","title":"Base","text":"<ul> <li>Initial Considerations</li> </ul> <p>We started by considering a base that could let us maximize the area permited by RCJ rules. We didn't make the exact dimensions because the rules state that the robot needs to fit smoothly into a cilinder of this diameter, so we had to leave a bit of leeway.</p> <p></p> <p>We also considered for a long time to use bases to support the PCBs. This was to keep them in a rigid piece of the robot and also to possibly give a padding to it to prevent vibrations in the PCBs. In the final implementation however, we ended up not using them because we never ended up using the padding, and connecting the PCBs alone wasn't as rigid but still maintained them correctly in place.</p> <p></p> <ul> <li>Implementation</li> </ul> <p>We ended up following the main ideas on our initial considerations, with some slight adjustments</p> <p></p> <p>The main base has the holes to support the following modules and pieces: the main PCB, the line PCBs, the voltage regulator, the IR ring, the dribbler, the ultrasonic sensor supports, the motor supports and zipties.</p> <p>Once these supports were added, the </p> <ul> <li>Problems</li> </ul> <p>The design did let us maximize the area, but it also caused a few issues. Although it did let us take more space and make it easier to block the ball, it conversely leaves a very small space to the . The 3 motor design didn't work very well and the implementation of a kicker wasn't possible.</p>"},{"location":"SoccerLightweight/2023/Mechanics/1.Robot_Lower_Design/#motors","title":"Motors","text":"<ul> <li>Initial Considerations</li> </ul> <p>We had the idea that to keep the motors in place we could use multiple zip ties to tie it onto the base, which would be easy to implement and lightweight. After initial testing, there was minimal jiggle to the motors and overall worked really well.</p> <ul> <li> <p>Implementation</p> </li> <li> <p>Problems</p> </li> </ul>"},{"location":"SoccerLightweight/2023/Mechanics/1.Robot_Lower_Design/#wheels","title":"Wheels","text":""},{"location":"SoccerLightweight/2023/Mechanics/1.Robot_Lower_Design/#dribbler","title":"Dribbler","text":""},{"location":"SoccerLightweight/2023/Mechanics/1.Robot_Lower_Design/#kicker","title":"Kicker","text":"<ul> <li>Initial considerations</li> </ul> <p>Our first approach to the kicker was that we wanted to focus on it later on in the implementations only as a possibility. The kicker was # 5 on our priority list, after the dribbler.</p> <p>We thought that the kicker was not very important, because of its weight and difficulty to implement. This is why we didn't focus on it since the beginning, and because of this we couldn't even attempt to implement it due to the lack of space.</p> <ul> <li> <p>Implementation</p> </li> <li> <p>Problem</p> </li> </ul> <p>The problem with implementing the kicker was that we didn't focus on it since the beginning and once we wanted to consider it, it would involve basically changing everything about the robot. The base design and the positioning of the motors made it difficult to fit it into our robot. If a kicker would be implemented, we would suggest to follow the base design suggested above.</p>"},{"location":"SoccerLightweight/2023/Mechanics/2.Robot_Upper_Design/","title":"Upper Design","text":""},{"location":"SoccerLightweight/2023/Mechanics/2.Robot_Upper_Design/#ir-ring-cover","title":"IR Ring Cover","text":"<ul> <li> <p>Initial Considerations</p> </li> <li> <p>Problems</p> </li> </ul>"},{"location":"SoccerLightweight/2023/Mechanics/2.Robot_Upper_Design/#adjustable-camera","title":"Adjustable Camera","text":"<ul> <li> <p>Initial Considerations</p> </li> <li> <p>Implementation</p> </li> <li> <p>Problems</p> </li> </ul>"},{"location":"SoccerLightweight/2023/Mechanics/2.Robot_Upper_Design/#mirror","title":"Mirror","text":"<ul> <li> <p>Initial Considerations</p> </li> <li> <p>Problems</p> </li> </ul>"},{"location":"SoccerLightweight/2023/Programming/General/","title":"General","text":""},{"location":"SoccerLightweight/2023/Programming/General/#tools","title":"Tools","text":"<p>The main tools used to program the robot are:</p> <ul> <li>Arduino CLI (Command based interface)</li> <li>Visual Studio Code (IDE)</li> <li>OpenMV IDE (IDE used to program the OpenMV camera)</li> </ul>"},{"location":"SoccerLightweight/2023/Programming/General/#_1","title":"General","text":""},{"location":"SoccerLightweight/2023/Programming/General/#strategy","title":"Strategy","text":"<p>During the regional competition we decided to have two attacking robots which had the same code due to time issues and other setbacks. Nonetheless, we learned that this was not a very good strategy, since both robots would sometimes crash with each other when searching for the ball, making scoring very difficult. Therefore, for the national competition we chose the strategy of developing an attacking and a defending robot. Ideally both robots would be able to change roles during the game, however the defending robot had the camera facing backwards, so this was not possible.</p> <p>It is also important to mention that the structure of the code worked as a state machine, advancing to different states until the previous one was completed. This was necessary since it was immportant to keep certain priorities. For example, for the attacking robot, it should first check that it isn't on a line before doing anything else.</p>"},{"location":"SoccerLightweight/2023/Programming/General/#algorithm","title":"Algorithm","text":""},{"location":"SoccerLightweight/2023/Programming/General/#attacking-robot","title":"Attacking Robot","text":"<p>The main objective of this robot was to gain possesion of the ball using the dribbler as fast as possible and then go towards the goal using vision. Therefore, the algorithm used is the following:</p> <p></p>"},{"location":"SoccerLightweight/2023/Programming/General/#defending-robot","title":"Defending Robot","text":"<p>On the other hand, the defending robot should always stay near the goal and go towards it if the ball is in a 20cm radius. The algorithm for this robot is shown in the following image:</p> <p></p>"},{"location":"SoccerLightweight/2023/Programming/IR_Detection/","title":"IR Detection","text":"<p>For the robot's movements, it was very important to know both the angle and the distance to the ball, so an IR ring was made with 12 IR receivers. However, before calculating these values, it was first necessary to obtain the pulse width of all TPS-58038s, which should be obtained during a ball emission cycle. However, this would imply a time of \\(833 \\mu s \\times  12\\) sensors. Therefore, a better approach was to obtain a reading from all the sensors all at once and repeat this process during the cycle \\((833 \\mu s)\\). Once the method was chosen, the distance and angle had to be calculated. Nonetheless, each had different methods:</p>"},{"location":"SoccerLightweight/2023/Programming/IR_Detection/#angle","title":"Angle","text":"<p>To obtain the angle towards the ball, there were two main options:</p> <ul> <li>Define the angle as the value of the sensor with the greatest pulse width (12 possible values)</li> <li>Vector calculation (360\u00b0)</li> </ul> <p>Since we wanted more precise values for the angle, we chose to use vector addition to get the resulting vector and hence, its angle. This was possible as each sensor had its unitary vector according to its position in the ring and the magnitud was the pulse width value. Therefore, after getting all sensor values in a cycle, the vectors where added using its x and y components, finally obtaining the angle as the inverse tangent of the result vector.</p>"},{"location":"SoccerLightweight/2023/Programming/IR_Detection/#distance","title":"Distance","text":"<p>For the distance, there were also different methods:</p> <ul> <li>Obtain the distance from the sensor with the greatest pulse width (its magnitud)</li> <li>Define the distance according to the number of phototransistors that detect the ball (active sensors)</li> <li>Vector calculation (resulting vector magnitud)</li> <li>Define the distance as the sum of the intensity measured by each sensor (combine method 1 and 2)</li> </ul> <p>Even though vector calculation seemed to be the best method, we faced several issues, since it was not very consistent and the result value usually varied within a range from 20 to 40. In addition, the first and second method were also ineficient by themselves, resulting also in a small range of distance. Therefore, both options were combined, which provided the best results, getting a distance with a range from 20 to 100.</p> <p>It is also important to mention, that we used the research of the Yunit team (2017) as a reference. There were also several tests and conclusions done, which can be found in the following document: IR Reasarch.</p>"},{"location":"SoccerLightweight/2023/Programming/IR_Detection/#detect-possession","title":"Detect Possession","text":"<p>Ideally, we wanted to use the IR Ring to check if the robots had possesion of the ball. However during some tests, we discovered that due to the bright colors, the goals could reflect infrared light emitted by the ball. Therefore, the IR Ring was placed on the robot at a height slightly above the goals to avoid reflections. Nonetheless, this did not allow us to get precise distance measurements when the ball was very close, so we could'nt know if we had possesion or not.  For this reason, another phototransistor was used with the only purpose to determine ball possesion. Similarly to the ring, the sensor counts the readings per cycle to determine the pulse width. However, to reduce noise and get more stable measurements, an Exponential Moving Average (EMA).</p> <pre><code>int detect() {\n  int pulseWidth = 0;\n  int deltaPulseWidth = 5;\n\n    const unsigned long startTime_us = micros();\n    do {       \n            filterAnalog.AddValue(analogRead(Constantes::analogo)); //Add value to filter\n            if(filterAnalog.GetLowPass() &gt; 700) {                   //Only consider the reading if the \n                pulseWidth += deltaPulseWidth;                      //infrarred emission is actually significant\n            }\n\n    } while((micros() - startTime_us) &lt; 833);\n\n  return pulseWidth;\n}\n</code></pre>"},{"location":"SoccerLightweight/2023/Programming/Line_Detection/","title":"Line Detection","text":"<p>To obtain the phototransistor values from the multiplexors, a function was created to obtain each value according to its binary code. In addition, we realized that each sensor would have different ranges when detecting green and white, so it was necessary to first make a calibration, which was different for the attacking and defending robot.</p>"},{"location":"SoccerLightweight/2023/Programming/Line_Detection/#attacking-robot","title":"Attacking Robot","text":"<p>Since this robot would go in all directions to search for the ball and score, it should never cross any white lines. Therefore, the phototransistors pcbs were used to estimate the angle on which the robot was touching the line to then move in the opposite direction. This was also complemented with ultrasonic sensors in order to avoid crashing with the walls, other robots and the ball itself (avoiding scoring on our goal).</p> <p></p> <p>The calibration for this robot was automatic and done when the robot started. Here, the robot would capture about 100 values for each sensor when standing on green to get the average measurement. Then, to check if there was a line, the robot would compare the current value with the average value and if the difference was greater than a threshold, then a line was detected and it was possible to see which phototransistor had seen it.</p>"},{"location":"SoccerLightweight/2023/Programming/Line_Detection/#defending-robot","title":"Defending Robot","text":"<p>The defending robot worked basically as a line follower, using two phototransistors (one from each side) in order to move horizontally. Additionally, it used the front phototransistor pcb to check if it had gone too far back, hence it would move forward. There were also three ultrasonic sensors used to avoid crashing with the walls and the ball.</p> <p></p> <p>For this robot, it was important to check the two phototransistors for line following before the match started, since the robot would move proportionally according to the sensors, looking to stay in the average value between white and green. Therefore, the calibration was done manually, by reading and then setting the white and green values for each sensor. The front sensor was calibrated automatically using the method explained in the attacking robot.</p>"},{"location":"SoccerLightweight/2023/Programming/Movement/","title":"Movement","text":""},{"location":"SoccerLightweight/2023/Programming/Movement/#holonomic-movemnt","title":"Holonomic movemnt","text":"<p>In order to take advantage of the holonomic drivetrain, the robot had to be able to move in any direction given the desired angle. Therefore, a kinematic model was used in order to determine the speed of each motor. In addition, part of our strategy required our robots to be always facing towards the goal, so the movement had to be corrected if the orientation changed. For this, the BNO-055 was used to get the current orientation and with a simplified PID controller (only P), the error was corrected. The following image shows the kinematic ecuations and corrections implemented:</p> <pre><code>double PID::getError(int target, int cur, int speed) {\n  error = abs(target - cur);\n  error = min(error, 100);  //Limit error to have a max value of 100\n  error *= kP;              //Constant of proportionality\n  return error;\n}\n</code></pre> <pre><code>void Motors::moveToAngle(int degree, int speed, int error) {\n  //Define each speed (values from 0-1)\n  float m1 = cos(((150 - degree) * PI / 180));\n  float m2 = cos(((30 - degree) * PI / 180));;\n  float m3 = cos(((270 - degree) * PI / 180));\n\n  //Multiply by given speed (0-255)\n  int speedA = (int(m1 * velocidad));\n  int speedB = (int(m2 * velocidad));\n  int speedC = (int(m3 * velocidad));\n\n  //Add error\n  speedA += error;\n  speedB += error;\n  speedC += error;\n\n  //Define absolute values\n  int abSpeedA = abs(speedA);\n  int abSpeedB = abs(speedB);\n  int abSpeedC = abs(speedC);\n\n  //Normalize values (to not exceed 255)\n  int maxSpeed = max(abSpeedA, max(abSpeedB, abSpeedC));\n  if (maxSpeed &gt; 255) {\n    abSpeedA = map(abSpeedA, 0, maxSpeed, 0, 255);\n    abSpeedB = map(abSpeedB, 0, maxSpeed, 0, 255);\n    abSpeedC = map(abSpeedC, 0, maxSpeed, 0, 255);\n  }\n\n  //Set speed to each motor\n  analogWrite(motor1.getPwmPin(), abSpeedA);\n  analogWrite(motor2.getPwmPin(), abSpeedB);\n  analogWrite(motor3.getPwmPin(), abSpeedC);\n\n  //Move motors depending on the direction needed\n  (speedA &gt;= 0) ? motor1.motorForward() : motor1.motorBackward();\n\n  (speedB &gt;= 0) ? motor2.motorForward() : motor2.motorBackward();\n\n  (speedC &gt;= 0) ? motor3.motorForward() : motor3.motorBackward();\n\n}\n</code></pre>"},{"location":"SoccerLightweight/2023/Programming/Movement/#attacking-robot","title":"Attacking robot","text":"<p>In order to take advantage of the HP motors, ideally, the robot should go as fast as possible, however, after a lot of testing, we found that the robot was not able to fully control the ball at high speeds, as it would usually push the ball out of bounds instead of getting it with the dribbler. Therefore, the speed was regulated depending on the distance to the ball (measured with the IR ring) using the following function:</p> <p></p> <p>\\(v(r) = 1.087 + 1/(r-11.5)\\), where \\(r\\) is the distance to the ball \\(\\in [0.00,10.0]\\) </p> <p>This equation was experimentally established with the goal of keeping speed at maximum until the robot gets very close to the ball, when the speed is quickly reduced.</p>"},{"location":"SoccerLightweight/2023/Programming/Movement/#defending-robot","title":"Defending robot","text":"<p>The idea for this robot was to keep it on the line line of the goal, always looking to keep the ball in front of it to block any goal attempts.Therefore, speed was regulated according to the angle and x-component to the ball. This meant that if the ball was in front of it, then it didn't have to move. However if the ball was far to the right or left, then speed had to be increased proportionally to the x-component of the ball, as shown in the following image:</p> <p></p>"},{"location":"SoccerLightweight/2023/Programming/Vision/","title":"Vision","text":""},{"location":"SoccerLightweight/2023/Programming/Vision/#target-detection","title":"Target detection","text":"<p>For goal detections, an Open MV H7 camera was used. Using the Open MV IDE, blob color detection was possible using micropyhton scripts. With this, the bounding box was identified and sent to the arduino. This measures were then used by both robots when going towards the goal or estimating the distance to the goal.</p>"},{"location":"SoccerLightweight/2023/Programming/Vision/#uart-communication","title":"UART communication","text":"<p>When sending information to the arduino, we faced several issues as the program would sometimes get stuck. We eventually realized that this was due to the protocol that we were using, as the buffer would sometimes receive an incomplete message and get an error when trying to process it. Therefore, to solve this issue, we changed the way to send and receive messages. In python the message was sent in the following format:</p> <pre><code>uart.write(f\"{tag},{x},{y},{w},{h}\\n\")\n</code></pre> <p>Here, the tag value was either an a or a b according to the color of the goal, then the x and y values were the center of the blob and w and h the width and heigth. this message was then received on the arduino on the Serial 3 port, using the following code:</p> <p><pre><code>void updateGoals() {\n\n  if (Serial3.available()) {\n    String input1 =  Serial3.readStringUntil('\\n');\n\n    if (input1[0] == 'a')\n      yellowGoal.update(input1);\n    else if (input1[0] == 'b')\n      blueGoal.update(input1);\n  }\n\n}\n</code></pre> For this first function, we received the bounding box and if the message began with an a, then the object yellow goal was updated and the same thing occured if it began with a b. It was important to first identify when the message began, since sometimes due to the buffer space, messages could be cut and begin with numbers or commas. </p> <p>Then, for the object update, the following code was used:</p> <p><pre><code>void Goal::update(String str) {\n  int arr[4];\n  String data = \"\";\n  int index = 0;\n\n  for (int i = 2; i &lt; str.length() &amp;&amp; index &lt; 4; i++) {\n\n    if (!(str[i] == ',')) {\n      data += str[i];\n    } else if (str[i] == ',' || i == str.length() - 1) {\n      arr[index++] = data.toInt();\n      data = \"\";\n    }\n\n    x = arr[0];\n    y = arr[1];\n    w = arr[2];\n    h = data.toInt();\n\n    area = w * h;\n\n  }\n\n}\n</code></pre> In this function, it was important for the loop to run until either the length of the string was done or the index got to three, which meant that 4 values were read. It was important to keep this counter, since once again, due to the buffer size, sometimes messages would be cut and combined with other values, which resulted in a longer string with more commas that would eventually cause an error.</p>"},{"location":"SoccerLightweight/2024/","title":"@SoccerLightweight - 2024","text":"<p>The main developments during 2024 with respect to previous years are the following:</p> <p>TODO: modify this.</p>"},{"location":"SoccerLightweight/2024/#mechanics","title":"Mechanics","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"SoccerLightweight/2024/#electronics","title":"Electronics","text":""},{"location":"SoccerLightweight/2024/#-jetson-nano","title":"- Jetson Nano","text":""},{"location":"SoccerLightweight/2024/#programming","title":"Programming","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"SoccerOpen/","title":"@SoccerOpen","text":"<p>[add description]</p>"},{"location":"SoccerOpen/#competition","title":"Competition","text":"<p>-</p>"},{"location":"SoccerOpen/#sections","title":"Sections","text":""},{"location":"SoccerOpen/#mechanics","title":"Mechanics","text":"<p>-</p>"},{"location":"SoccerOpen/#electronics","title":"Electronics","text":"<p>-</p>"},{"location":"SoccerOpen/#programming","title":"Programming","text":"<p>-</p>"},{"location":"SoccerOpen/2024/","title":"@SoccerOpen - 2024","text":"<p>The main developments during 2024 with respect to previous years are the following:</p> <p>TODO: modify this.</p>"},{"location":"SoccerOpen/2024/#mechanics","title":"Mechanics","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"SoccerOpen/2024/#electronics","title":"Electronics","text":""},{"location":"SoccerOpen/2024/#-jetson-nano","title":"- Jetson Nano","text":""},{"location":"SoccerOpen/2024/#programming","title":"Programming","text":"<ul> <li>Jetson Nano</li> <li>d</li> </ul>"},{"location":"home/","title":"@Home","text":"<p>Welcome to the @Home competition documentation of RoBorregos, Robotics Representative Group of Tecnol\u00f3gico de Monterrey. This documentation serves as a guide for our project, which focuses on developing multiproposal software modules for the RoboCup@Home competition. Our goal is to create a comprehensive software architecture that enables robots to perform various tasks that humans typically handle on a daily basis.</p>"},{"location":"home/#about-the-competition","title":"About the Competition","text":"<p>The RoboCup@Home league is a part of the RoboCup initiative and is the largest annual competition for autonomous service robots. Its goal is to develop service and assistive robot technology for future domestic applications. The competition evaluates robots abilities in a realistic home environment through benchmark tests.  The focus areas include: </p> <ul> <li>Human-Robot Interaction</li> <li>Navigation and Mapping</li> <li>Computer Vision</li> <li>Object Manipulation</li> <li>System Integration</li> </ul> <p>The RoboCup@Home competition is divided into three categories: the Domestic Standard Platform League (DSPL), the Social Standard Platform League (SSPL), and the Open Platform League (OPL). Our team competes in the Open Platform League.</p> <p>In the Open Platform League, we have the opportunity to explore and showcase our innovations and advancements without limitations on the choice of hardware platforms. This category allows us to push the boundaries of robotics and demonstrate our capabilities in a variety of areas.</p>"},{"location":"home/#project-objective","title":"Project Objective","text":"<p>The main objective  is to actively explore and integrate state-of-the-art technologies in the field of robotics. Through our participation in the RoboCup@Home competition's Open Platform League, we aim to demonstrate our knowledge, skills, and innovative thinking in developing advanced robotic systems. By leveraging cutting-edge hardware and software, we strive to push the boundaries of current technology and create innovative solutions for real-world problems.</p>"},{"location":"home/#contributing","title":"Contributing","text":"<p>We welcome contributions from the open-source community. If you are interested in contributing to our project, please refer to the \"Contributing\" section for guidelines on how to get involved.</p>"},{"location":"home/#contact-us","title":"Contact Us","text":"<p>If you have any questions, suggestions, or feedback regarding our project or this documentation, please feel free to reach out to us. You can find our contact information in the \"Contact Us\" section.</p> <p>We hope you find our documentation helpful, and we wish you success in exploring and utilizing our software modules for the RoboCup@Home competition.</p>"},{"location":"home/2022-Jun%202023/","title":"Developments from 2022-2023","text":""},{"location":"home/2022-Jun%202023/#electronics-and-control","title":"Electronics and Control","text":"<p>Designed and manufactured a PCB for handling the SCARA robot's motors and arm. </p>"},{"location":"home/2022-Jun%202023/#mechanics","title":"Mechanics","text":"<p>Integrated the Xarm5 robot arm with the Dashgo B1 mobile base, as well as developed a custom gripper and electronics protections.</p>"},{"location":"home/2022-Jun%202023/#human-robot-interaction","title":"Human Robot Interaction","text":""},{"location":"home/2022-Jun%202023/#speech-recognition","title":"Speech Recognition","text":"<p>Developed a ROS node for speech recognition using Azure's Speech Service.</p>"},{"location":"home/2022-Jun%202023/#action-interpretation","title":"Action Interpretation","text":"<p>Developed a ROS node for action interpretation using custom embeddings and a GPT-3 model.</p>"},{"location":"home/2022-Jun%202023/#integration-and-networks","title":"Integration and Networks","text":""},{"location":"home/2022-Jun%202023/#ros","title":"ROS","text":"<p>Developed the main engines for the competition tasks, including General Purpose Service Robot, Receptionist and Carry my Luggage.</p>"},{"location":"home/2022-Jun%202023/#computers","title":"Computers","text":"<p>Integrated a Jetson AGX Xavier as the robot's main computer as well as a Jetson Nano for the mobile base.</p>"},{"location":"home/2022-Jun%202023/#networking","title":"Networking","text":"<p>Integrated the on-robot computers, Xarm and base within a wired network, as well as a wireless router for full internet access and for external computing resources.</p>"},{"location":"home/2022-Jun%202023/#computer-vision","title":"Computer Vision","text":""},{"location":"home/2022-Jun%202023/#pose-estimation","title":"Pose Estimation","text":"<p>Developed a pose estimation ROS node, mainly used for detecting simple actions.</p>"},{"location":"home/2022-Jun%202023/#object-detection","title":"Object Detection","text":"<p>Switched to YOLOv5 for object detection and developed a method for fast and semi authomatic dataset generation.</p>"},{"location":"home/2022-Jun%202023/#human-analysis","title":"Human Analysis","text":"<p>Used DeepFace for face recognition and analysis as well as object detection models for clothing description.</p>"},{"location":"home/2022-Jun%202023/Team%20Members/","title":"Team Members 2022-2023","text":""},{"location":"home/2022-Jun%202023/Team%20Members/#electronics-and-control","title":"Electronics and Control","text":"<ul> <li>Leonardo Llanas</li> <li>David V\u00e1zquez</li> </ul>"},{"location":"home/2022-Jun%202023/Team%20Members/#mechanics","title":"Mechanics","text":"<ul> <li>Leonardo S\u00e1nchez</li> <li>Ignacio Maldonado</li> <li>Karyme Ezrr\u00e9</li> <li>Lorena Mart\u00ednez</li> <li>Michell Sarmiento</li> <li>Jos\u00e9 Fragoso</li> </ul>"},{"location":"home/2022-Jun%202023/Team%20Members/#integration-and-networks","title":"Integration and Networks","text":"<ul> <li>Kevin Vega</li> <li>Jos\u00e9 Cisneros</li> </ul>"},{"location":"home/2022-Jun%202023/Team%20Members/#navigation","title":"Navigation","text":"<ul> <li>Bryan M\u00e1rquez</li> <li>Edison Altamirano</li> <li>Alexis Chapa</li> </ul>"},{"location":"home/2022-Jun%202023/Team%20Members/#manipulation","title":"Manipulation","text":"<ul> <li>Jos\u00e9 Cisneros</li> <li>Ad\u00e1n Flores</li> <li>Erasmo Villarreal</li> </ul>"},{"location":"home/2022-Jun%202023/Team%20Members/#computer-vision","title":"Computer Vision","text":"<ul> <li>Aldo Samaniego (Project Manager)</li> <li>Iv\u00e1n Romero (Project Manager)</li> <li>Emiliano Flores</li> <li>Jonatan de la Rosa</li> <li>Jamir Leal</li> </ul>"},{"location":"home/2022-Jun%202023/Computer%20Vision/","title":"Computer Vision","text":"<p>The different Challenges that the @Home Competition has, requires several computer vision techniques to be solved. This section will show some of the most used techniques in the competition.</p> <p>First of all, it's important to know the general solution of the Vision Modules for the @Home Competition. We can divide the Vision Modules in two main categories:</p> <ul> <li>Object Detection</li> <li>Human Analysis</li> </ul>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Human%20Analysis/","title":"Overview","text":"<p>The Human Analysis Module represent a different challenge in terms of logic in the sense of how to manage and when to call inferences.</p> <p>The Submodules or Tasks that are part of this module are: - Face Detection and Recognition - Human Pose Estimation - Human Attributes</p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Human%20Analysis/Pose%20Estimation/","title":"Pose Estimation with MediaPipe","text":"<p>Pose estimation was implemented using MediaPipe for the RoboCup 2022 @Home Simulation competition. The pose estimation algorithm is based on the MediaPipe Pose solution. </p> <p>It's very simple, acurate and fast. It's also very easy to use, since it's a pre-trained model that can be used directly.</p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Human%20Analysis/Pose%20Estimation/#how-to-use-it","title":"How to use it","text":"<p>First of all, you need to install MediaPipe. You can do it by running the following command:</p> <pre><code>pip install mediapipe\n</code></pre> <p>Then, you can use the following code to get the pose estimation:</p> <pre><code>import mediapipe as mp\n\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n\n# Opening the image source to be used\nimage = cv2.imread(\"image.jpg\")\n\n# Calling the pose detection model\nwith mp_pose.Pose(\n        min_detection_confidence=0.5,\n        min_tracking_confidence=0.5) as pose:\n    # Detecting the pose with the image\n    poseResult = pose.process(image)\n</code></pre> <p>As a result, you'll have a <code>poseResult</code> array of points. That each point represent a joint of the body, as shown in the following image:</p> <p></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Human%20Analysis/Pose%20Estimation/#using-pose-estimation-with-webcam","title":"Using pose estimation with webcam","text":"<p>You can also use pose estimation with a webcam to get streamed video. You can use the following code to do it:</p> <pre><code>import mediapipe as mp\n\nimport cv2\n\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n\n# Calling the solution for image drawing from MediaPipe\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n\n\n# Opening the webcam\ncap = cv2.VideoCapture(0)\n\n# Calling the pose detection model\nwith mp_pose.Pose(\n        min_detection_confidence=0.5,\n        min_tracking_confidence=0.5) as pose:\n    # Looping through the webcam frames\n    while cap.isOpened():\n        # Reading the webcam frame\n        success, image = cap.read()\n        if success:\n\n            # Managing the webcam frame\n            image.flags.writeable = False\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n            # Detecting the pose with the image\n            results = pose.process(image)\n\n            # Drawing the pose detection results\n            image.flags.writeable = True\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n            mp_drawing.draw_landmarks(\n                image,\n                results.pose_landmarks,\n                mp_pose.POSE_CONNECTIONS,\n                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n            cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\n            if cv2.waitKey(5) &amp; 0xFF == 27:\n                break\ncap.release()\n</code></pre> <p>As a result, you'll not only be able to get the pose estimation array. but also the stream with the drawing of the pose estimation.</p> <p>Example:</p> <p></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Human%20Analysis/Pose%20Estimation/#using-pose-estimation-with-ros","title":"Using pose estimation with ROS","text":"<p>You can receive the image source from a ROS topic. You can use the following code to do it:</p> <pre><code>import mediapipe as mp\nfrom time import sleep\nfrom typing import Tuple\nimport cv2\nimport numpy as np\nimport rospy\nfrom cv_bridge import CvBridge\nfrom sensor_msgs.msg import Image\n\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n\n# Calling the solution for image drawing from MediaPipe\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n\n# Declaring the CvBridge for image conversion from ROS to OpenCV\nbridge = CvBridge()\n\n# Declaring the image and its callback for the ROS topic\nimageReceved = None\ndef image_callback(data):\n    global imageReceved\n    imageReceved = data\n\n# Initializing the ROS node\nrospy.init_node('ImageRecever', anonymous=True)\n\n# Subscribing to the ROS topic\nimageSub = rospy.Subscriber(\n    \"/hsrb/head_center_camera/image_raw\", Image, image_callback)\n\n# Calling the pose detection model\nwith mp_pose.Pose(\n        min_detection_confidence=0.5,\n        min_tracking_confidence=0.5) as pose:\n    # Looping through the image frames\n    while not rospy.is_shutdown():\n        if imageReceved is not None:\n            # Converting the ROS image to OpenCV\n            image = bridge.imgmsg_to_cv2(imageReceved, \"rgb8\")\n\n            # Detecting the pose with the image\n            image.flags.writeable = False\n            results = pose.process(image)\n\n            # Drawing the pose detection results\n            image.flags.writeable = True\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n            mp_drawing.draw_landmarks(\n                image,\n                results.pose_landmarks,\n                mp_pose.POSE_CONNECTIONS,\n                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n\n\n            cv2.imshow('MediaPipe Pose', image)\n            if cv2.waitKey(5) &amp; 0xFF == 27:\n                break\n        else:\n            print(\"Image not recived\")\n        sleep(1)\n</code></pre> <p>Here is an example of the result:</p> <p></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/","title":"Overview","text":"<p>The Object Detection module represent the challenge of identify and locate objects in the environment. This module is used in computer vision and image processing to detect objects in an image or video sequence. The goal is to identify the object and its location within the image or video frame. The module uses various techniques such as feature extraction, object recognition, and machine learning algorithms to achieve this task.</p> <p></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/#challenges-and-tasks","title":"Challenges and Tasks","text":"<p>The Object Detection module is focused on produce a highly accurate while fast object detection system. The first approach for this in the first year of participation, was to use a pre-trained model based on the Object Detection API from Tensorflow. This model was trained with the COCO dataset, which contains at most 4 different classes of objects. The process was to generate the dataset from a manually labeled dataset of images, and train the model with it. </p> <p></p> <p>This approach had several problems listed below:</p> <ul> <li>The model was not accurate enough to be very confident in the detections.</li> <li>The model was not fast enough to be used in real time.</li> <li>The classes of objects where limited and if we wanted to add more classes, we had to use a different model.</li> <li>The labeling process was very time consuming and tedious.</li> </ul> <p>To solve this problems, we decided to use a different approach. We decided to make two big improvements to the system. The first one was to automate the dataset generation process, making it faster and easier to generate and improving the dataset quality. The second one was to experiment with different pre-trained models and different Object Detection Algorithms. With the use of different models and algorithms, we conclude that the Yolov5 model outperformed the other models in terms of accuracy and speed.</p> <ul> <li>Dataset Generation</li> <li>Customly Trained Object Detection Models<ul> <li>Yolov5</li> <li>NVIDIA TAO Toolkit</li> <li>Tensorflow Object Detection API</li> <li>Tensorflow Lite Model Maker</li> </ul> </li> </ul>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Dataset%20Automatization/","title":"Dataset Automatization","text":"<p>Dataset generation is the process of collecting and labelling the images according to the objects the detection model will be trained for. It is a slow and often tedious process, as not only hundreds or thousands of images have to be taken, but the labelling process is even slower, and it becomes more difficult as the number of objects or precision objectives increase.</p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Dataset%20Automatization/#solution-concept","title":"Solution concept","text":"<p>The solution implemented for the 2023 @Home competition was to automatize both the generation of images and their labelling, both for object detection and segmentation. </p> <p>This process consists of:</p> <ul> <li>Taking images of the objects the model will be trained for, ideally on as many angles and environmental conditions as possible. </li> <li>Extracting the objects from their background, producing a dataset of PNG backgroundless images for every object to be detected.</li> <li>Inserting the objects on a dataset of backgrounds (both manually taken and/or from random datasets), using image augmentation to generate as many variations of the objects and their surroundings as possible (color correction, size, obstructions, rotation)</li> <li>Saving the position where the objects are inserted to autogenerate the labelling information alongside the whole dataset.</li> <li>Exporting the dataset to the desired format, according to the training model to be used.</li> </ul>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Dataset%20Automatization/#object-dataset","title":"Object dataset","text":"<p>Object images taken should have the object on clear and full view, and effort should be taken to get as many angles and environmental conditions (like lightning conditions, color temperature, shadows) as possible. The scripts used during 2023 @Home competion are available at:</p> <pre><code>git clone https://github.com/EmilianoHFlores/synthDataset.git\n</code></pre> <p>With the scripts provided in the dataset, the object is cut to be used for the creation of the synthetic dataset. </p> <ul> <li>First the image is detected using available YOLOv5 models (which remains one of the most important improvements to be made, as some objects could prove difficult to find an adequate model to be detected). Then the object is segmented and cut using the Segment-anything model from META. </li> </ul> <p></p> <p>After cut:</p> <p></p> <p>With this photo, the object remains at its original position in the image (which can also be used as labels for training with real images).</p> <ul> <li>Then, the image is cut to the object size, which will be the one used in the creation of datasets.</li> </ul> <p></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Dataset%20Automatization/#dataset-creation","title":"Dataset creation","text":"<p>Using a dataset of various backgrounds (including the area of the competition and random datasets obtained online), the notebook included in the repository constructs the dataset inserting the images from all the classes stated on it. </p> <p>It then exports it in a format readable for the training models, currently with a version exporting annotations as a COCO json format and in the YOLOv5 one. The notebook exports the COCO format with segmentation, so it can be used with models that support training with segmentation labels.</p> <p>Example of an image produced: </p> <p>Segmentation visualized: </p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Dataset%20Automatization/#results-obtained","title":"Results Obtained","text":"<p>While images from the datasets created may sometimes look curious and the objects shown on positions and places they would never be found, this process was shown to be succesful to a great extent. An example of this was how one of the first models was trained, on which only backgrounds from the table the model was going to be detecting on were used. The results of this model, while accurate on that space, struggled on other areas:</p> <p></p> <p>After retraining with a dataset using diverse backgrounds (both taken from the work area and others), this problem was mostly solved: </p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/TFLite%20Model%20Maker/","title":"TensorFlow Lite Model Maker","text":"<p>The TensorFlow Lite Model Maker is a library that simplifies the process of training a TensorFlow Lite model using transfer learning. It's very simple to use and it's very useful for a fast prototyping of a model. </p> <p>The Object Detection Module of Model Maker, has different EfficientNet versions</p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/TFLite%20Model%20Maker/#enviroment","title":"Enviroment","text":"<p>The requirements of the TensorFlow Lite Model Maker could be difficult to reproduce in different enviroments. To avoid this, we recommend to use the Dockerfile developed in the proyect: Dockerfile</p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/TFLite%20Model%20Maker/#usage","title":"Usage","text":"<p>As mentioned before, the TensorFlow Lite Model Maker is very simple to use. The following steps are the ones that we used to train a model for the Object Detection Module:</p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/TFLite%20Model%20Maker/#dataset","title":"Dataset","text":"<p>The dataset must be in a CSV format, with the following columns format (without headers): <pre><code>Usage, Path, Label, XMin, XMax, , , YMin, YMax\n</code></pre> Sample annotation:  <pre><code>TEST,dataset/test_set/0.jpg,pringles,0.1572265625,0.00390625,,,0.2587890625,0.15885416666666666\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/TFLite%20Model%20Maker/#training","title":"Training","text":"<p>Import the required libraries: <pre><code>import numpy as np\nimport tensorflow as tf\nfrom tflite_model_maker import model_spec\nfrom tflite_model_maker import object_detector\n</code></pre></p> <p>Set Logging Level: <pre><code>tf.get_logger().setLevel('ERROR')\nfrom absl import logging\nlogging.set_verbosity(logging.ERROR)\n</code></pre></p> <p>Load the dataset: <pre><code>train_data, validation_data, test_data = object_detector.DataLoader.from_csv('dataset.csv')\n</code></pre></p> <p>Set the model specification: <pre><code>spec = model_spec.get('efficientdet_lite4')\n</code></pre></p> <p>Train the model: <pre><code>model = object_detector.create(train_data, model_spec=spec, batch_size=8, epochs=100, train_whole_model=True, validation_data=validation_data)\n</code></pre></p> <p>Evaluate the model: <pre><code>model.evaluate(test_data)\n</code></pre></p> <p>Export the model: <pre><code>model.export(export_dir='.')\n</code></pre> You could especify the export method for the model in other formats, like TFLite or SavedModel:</p> <p>TFLite: <pre><code>model.export(export_dir='.', tflite_filename='model.tflite', quantization_config=QuantizationConfig.for_float16(), export_format=[ExportFormat.TFLITE])\n</code></pre></p> <p>SavedModel: <pre><code>model.export(export_dir='.', export_format=[ExportFormat.SAVED_MODEL, ExportFormat.LABEL])\n</code></pre></p> <p>Sample results:</p> <p></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/yolov5/","title":"YOLOv5","text":"<p>YOLOv5 is a recent model developed by Ultralytics. It is a simple use, fast and accurate object detection model. It also allows for training with different weights, which enables the user to choose their model size and resource cost.</p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/yolov5/#enviroment","title":"Enviroment","text":"<p>Enabling CUDA capabilities for both running the models and training them can be conflicting with most environments. Ultralytics has provided a CUDA compatible dockerfile over which both inference and training can be easily performed.  <pre><code>sudo docker pull ultralytics/yolov5:latest\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/yolov5/#run-with-gpu-support","title":"Run with GPU support","text":"<p>The following command will enable both GPU support (without any further setup) and will mount the current terminal directory, so it can access user files for both detect and training results to be accessed and stored.</p> <pre><code>docker run --ipc=host -it --gpus all -v $(pwd):/usr/src/app ultralytics/yolov5:latest\n</code></pre> <p>To use a specific GPU device: <pre><code>docker run --ipc=host -it --gpus device=1 -v $(pwd):/usr/src/app ultralytics/yolov5:latest\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/yolov5/#usage","title":"Usage","text":"<p>The YOLOv5 repository from ultralytics has an extensive documentation on detecting and training. It also provides python scripts that allow for easy training and detection tests. To train a model on YOLOv5:</p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/yolov5/#dataset","title":"Dataset","text":"<p>Datasets for training have a specific format that consists of a folder with the following structure: <pre><code>Dataset\n\u2502   data.yaml\n\u2514\u2500\u2500\u2500 train\n\u2502   \u2514\u2500\u2500\u2500 images\n|   |      image1.png\n|   |      image2.png\n|   |      ...\n|   \u2514\u2500\u2500\u2500labels\n|          image1.txt\n|          image2.txt\n|          ...\n\u2514\u2500\u2500\u2500 test\n|       ...\n\u2514\u2500\u2500\u2500 validate\n        ...\n</code></pre> Where data.yaml contains the information regarding the number of classes, names and identifiers of each class and the locations of the train, test and validation folders. It follows the structure (assuming names of folders as the above example): <pre><code>names:\n- nameofclass1\n- nameofclass2\n...\nnc: (numberofclasses)\ntest: ../test/images\ntrain:  ../train/images\nval: ../validate/images\n</code></pre> The annotations for each image labels are contained within the .txt file of the same name, in the labels folder, using the following format:</p> <p><pre><code>0 x_center y_center width height\n</code></pre> Where all four values from the bounding boxes are given in percentage (decimal) of their position in the image, not in pixels.</p> <p>Sample annotation: <pre><code>5 0.4830163043478261 0.303125 0.020380434782608696 0.07291666666666667\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Computer%20Vision/Object%20Detection/Custom%20Models/yolov5/#training","title":"Training","text":"<p>Training on YOLOv5 is done with a single python script that accepts several arguments regarding its cofiguration and the dataset used for it.  For this,  the YOLOv5 repository has to be cloned locally: <pre><code>git clone  https://github.com/ultralytics/yolov5\n</code></pre> Then, mount the docker image on a folder with access to the repository, and cd to the yolov5 folder. The train.py script has many arguments to be edited, but the recommended ones are shown on the following sample command: <pre><code>python train.py --img 640 --batch 16 --epochs 300 --data $(pwd)/pathToDataset/dataset1/data.yaml --weights yolov5m.pt --name trainTest --cache --patience 50\n</code></pre> This script sets the recommended image and batch sizes, and allows both for a maximum of 300 epochs and the patience (how many epochs without accuracy improvement will be allowed) on 50. The --data should be the location of the data.yaml file located inside the dataset, structured accordingly to the previous section. </p> <p>The weights of the model will define the size of the trained model's file, as well as the requirements it will ask for the computer on which the inference will run. The repository currently has 6 default weights that can be used: <pre><code>yolov5n.pt NANO\nyolov5s.pt SMALL\nyolov5m.pt MEDIUM\nyolov5l.pt LARGE\nyolov5x.pt EXTRA LARGE\n</code></pre> With the following precision and speed each:  With testing, yolov5m has proven of enough accuracy while occupying less than 2GB of VRAM allocated on runtime.</p> <p>After training has finished, it will create a new folder on runs/train. There the created models will be contained on the weights folder, including the one generated on the last epoch of the training and the one with the best average precision. The folder will also include the results statistics in a graphical and as a CSV file.</p> <p>To validate and test the model, it is recommended to use the tutorial.ipynb notebook, which already contains the commands to run both the detect.py and validate.py scripts. Both will generate folders on /runs with the results.</p> <p>Sample results:</p> <p></p> <p>Detection was made using the YOLO ROS Wrapper available at: <pre><code>git clone https://github.com/mats-robotics/yolov5_ros.git\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/","title":"Overview","text":"<p>Electronics and Control is the area in charge of the design, construction and application of all the circuits at any level, but also of the control of the movements in the robot at a low level, hence it interacts directly with the mechanics area.</p>"},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/#sections","title":"Sections","text":"<ul> <li>Electronics</li> <li>Control</li> </ul>"},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/Control/","title":"Control","text":"<p>On the this side, we mainly develop reliable control, for the basic movements of the robot, so that the software developers can access easily to this features of the robot.</p> <p>First of all we need to explain the communication between the main computer and the microcontrollers. We can understand it more easy with these diagram:</p> <p></p> <p>The serial communication is done thru (pyserial)[https://pypi.org/project/pyserial/]. Thelogic of controll is that the microcontroller is running constantly a PID control system</p>"},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/Electronics/","title":"Electronics","text":""},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/Electronics/#main-objective","title":"Main objective","text":"Electronics development in Roborregos consists in the ideation, design, evaluation, construction and implementation of solid electronic systems which must be able to apply proficient control over different actuators and allow robust communication among sensors, modules and microcontrollers."},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/Electronics/#metodology","title":"Metodology","text":"In order to build a solid electronic system it is compulsory to develop custom PCBs (Printed Circuit Boards). There are multiple steps that must be followed in order to avoid critical mistakes in the final PCB design, and though there is no a single way to make a PCB, these fundamentals steps must be followed without skipping under any circumstances:  <ul> <li>Research every component involved in the system</li> </ul>  Even before drawing connections or schematics, it is obligatory to research every component that will be used. Application notes or technical PDFs are the main source of information to understand how the selected devices work, and what must be considered to ensure their best operational conditions. Also, this is the only way to prevent accidents related to power supplies, bad connections, or inappropiate use of components.  <ul> <li>Make a diagram of connections</li> </ul>  After researching about the components that will be used is crucial to specify in a graphic way how they are going to be connected with each other. This ensures that everyone involved in the project understands how the relationship between the components and how to avoid hazards from incorrect connections.  <ul> <li>Test the conections in a protoboard or perforated board</li> </ul>  No PCB is ordered without previous testing because of unexpected behaviors that componentes may have due to the real-world conditions involved in the circuitry. Though most of the fundamental calculations can be done manually (like the total amperage consumption or voltage drops in certain components) there's simply no way of proving that a design will work if there is no proof of it's functionality in a protoboard or a perforated board.     Building the desired circuit and testing it it's the only way to know with precision if it's going to work in a PCB, and also, which issues may have under certain conditions. However, this rule may not apply to the implementation of MCUs (microcontrollers) in SMD packages due to physical limitations or other specific components which might not be able to be tested in conventional ways.    <ul> <li>Ask someone else to evaluate your design or to check for improvements</li> </ul>  It is important to get every PCB evaluated by at least another person which understands its purpose and functionality. Ideally, in @home every electronic must know their PCBs and the principles to determine their quality, so always take in consideration that there's room for improvement and the ideas that might come from discussion will be beneficial for the robot's development."},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/Electronics/#considerations","title":"Considerations","text":"Each PCB must comply with specific requirements to avoid supply, shortcut and efficiency issues according to it's purpose.      The main process followed for electronics application comes like this: Desgin/Circuit building/Connection.  First of all the design of the connections between modules.  In order to get a full control of the robot it is fundamental to understand how it is composed  The scara-based robot has omnidirectional movement which comes from the usage of 4 mecanum wheels, powered by 4 Pololu's HP motors.       Then for the design process we use EasyEda .   For the building process, we use the next standars for general purposes:    For the connectors standars we have:    The cable selection was made giving the next parameters:"},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/Boards/Boards/","title":"Boards","text":""},{"location":"home/2022-Jun%202023/Electronics%20and%20Control/Boards/Boards/#boards-developed-at-the-moment","title":"Boards developed at the moment","text":"<ul> <li>Base controller board</li> </ul> <p>This board main goal is to control the motors of the base, which are 4 DC 12v/6v motors with 270:1 gear box reduction. Model CQGB37Y001, these motors come with magentic encoders already installed. So the board has one ATMega 2560 has the microcontroller, it has tow voltage supplies, one of 12v that can be conneceted thru a XT-60 connector or a terminal block.</p> <p></p> <p>And also has a 5v voltage supply provided by the serial port which also allows communication with the main computer. This port is supported by a female microsUSB that is conected to the FT232RQ chip. This chip also allows to use the a FTDI to acces the microcontroller. </p> <p>Voltage can be interrupted manually by the headers shown bellow.</p> <p></p> <p>For the drivers we have two Dual MC33926 Motor Driver Carrier, so each one controls two motors, giving us the total 4 motors we need. This driver was selected because its reliable protection features and its voltage-current range. </p> <p>For the orientation feedback we have the adafruit IMU sensor BNO055.</p> <p></p> <p>This board also has some miscelanous features such as extra voltage Outputs (5v/12V), I2C pins for communication, bootloader pins, and 6 extra digital pins for general purposes(emergency stop for example), indicator LEDs and a reset button.</p> <ul> <li>SCARA controller and board</li> </ul> <p>This board is also based on the ATMega 2560 microcontroller but uses instead an Arduino Mega board plugged, this was donde like this because of components stock issues but it is going to be updated to have an SMD chip. The board purpose is to controll all the other motors that are in the robot. Listing the motors we will have the next list. Steppers -Two NEMA17 steppers needed by the SCARA arm. -One NEMA17 for the elevator -And one NEMA17 for an extra degree of freedom for the gripper.</p> <p>Giving us a total of four stepper motors plus 2 extra ones in case they are needed. So our board supports a total of six stepper motors, that are control by 6 drivers TMC2208. For the pinout to the drivers you can download the next Arduino file or watch it directly in the PCB.</p> <p>This board also supports feedback for the stepper motors, thru the AS5600 contactless potentiometer, which allows to know the position of the motor using a magnet fixed to the back of the motor. </p> <p></p> <p>This model of encoder recquired the desgin of a housing so that the encoder and the motor will stay together. It uses communication via I2C, so for the correct use of this encoders it is also recquired to implement a multiplexor so that you can read multiple encoders. </p> <p></p> <p>Now for the servo motors, we mainly use two models, the 1245MG when more torque is needed, or the smaller but less current demanding MG995 This board supports up to six 7v servomotors and the pinout to the motors is also declared on the Arduino file shown before.</p> <p>And also has some miscelanous features such as extra voltage Outputs (5v/7v/12V), FTDI pins for communication, bootloader pins, indicator LEDS and voltage interruption headers.</p> <ul> <li>Power board</li> </ul> <p>Lastly for the power supply board we just desgined a distribution circuit that can be interrupted via a Start and Stop buttons. The interruption is made by a two relay latch circuit.</p> <p></p> <p>NOTE: The connection in the normally close and normally open  contacts is done like this because the blueprint and the real pinout is reversed. So in the real circuit, the connections go to the normally open contact.</p> <p>Mainly in this board we just receive the main power supply and regulate it thru 3 DC Buck converters, each one for one of the operating voltages (5v, 7v or 12v). This board also has extra voltage outputs (5v/7v/12v).  IMPORTANT: this board is in revision due to errors found during its implementation.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/","title":"Human Robot Interaction","text":"<p>Human-Robot Interaction (HRI) refers to the study and design of interactions between humans and robots. It encompasses the development of technologies, interfaces, and systems that enable effective communication, collaboration, and cooperation between humans and robots. HRI aims to create intuitive and natural ways for humans to interact with robots, allowing for seamless integration of robots into various domains such as healthcare, manufacturing, entertainment, and personal assistance.</p> <p>In Roborregos the previous work was develop only considering speech, a subset of HRI that involves the use of speech recognition and speech synthesis to enable verbal communication between humans and robots. Also the hardware setup of the area and the software modules that are used were developed </p> <p>The last implementation to get the entities of the text was using GPT-3 API.</p> <p>Also Human Analysis area was develop, a basis for the development of natural behavior between humans and robots using computer vision techniques.</p> <p>Currently we are working on the development of the area considering features such as gestures, facial expressions, and body language recognition to enhance the human-robot interaction experience. By incorporating these features, robots can better understand and respond to non-verbal cues from humans, leading to more effective communication and collaboration.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/","title":"Overview","text":"<p>The area of speech have had different implementations using different and software solutions. Mainly the area is composed by the two components: </p> <ul> <li>Speech To Text</li> <li>Text To Speech</li> </ul>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#important-topics","title":"Important topics","text":"<p>You can run all the nodes in yor machine or in the robot. The simplest usage of speech area is with three topics:</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#inputaudioactive","title":"InputAudioActive","text":"<p>Used as a flag to start listening to the user. It is a Bool topic, when it is True the robot is listening to the user, when it is False the robot is not listening to the user. Commonly used as:     <pre><code>self.speech_enable = rospy.Publisher(\"inputAudioActive\", Bool, queue_size=10)\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#saying","title":"Saying","text":"<p>Used to know the status of the robot, when it is talking or not. It is a Bool topic, when it is True the robot is talking, when it is False the robot is not talking. Commonly used as:     <pre><code>self.say_listener = rospy.Subscriber(\"saying\", Bool, self.say_callback)\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#robottext","title":"RobotText","text":"<p>Used to send the text that the robot will say. It is a String topic, when it is published the robot will say the text. Commonly used as:     <pre><code>self.robot_text = rospy.Publisher(\"robot_text\", String, queue_size=10)\n</code></pre></p> <p>The regular function of a say action is the following:</p> <p><pre><code>def say(self, text):\n    time.sleep(1)   \n    if self.saying == False:\n        self.saying == True\n        self.say_publisher.publish(text)\n    time.sleep(len(text)/8 )    \n</code></pre> That becasue of the natural delay of the robot to say the text depending on the length of the text.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#rawinput","title":"RawInput","text":"<p>Is the processed text that the robot understood from the user. It is a String topic, when it is published the robot will say the text. Commonly used as:     <pre><code>self.raw_input_listener = rospy.Subscriber(\"rawInput\", String, self.raw_input_callback)\n</code></pre></p> <p>With a simple usage of the speech area you can use the following code to get the text from the user:</p> <pre><code>def input_callback(self, msg):\n    self.inputText = msg.inputText\n    rospy.logdebug(\"I heard: \" + self.inputText)\n</code></pre>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#usage","title":"Usage","text":"<p>This are a basic example of implementation in python of the speech area.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#promts","title":"Promts","text":"<p>Promts are simply a dictionary of text that the robot can say. It is a dictionary of String topic, when it is published the robot will say the text. Commonly used as: <pre><code>promts = {\n\"start\": \"Let's start General Purpose Service Robot \",\n\"wait\": \"I am waiting for someone\",\n\"self_intro\": \"Hello, I'm homie, your personal assistant.\",\n\"ask_comm\": \"What do you want me to do ?\",\n\"ask_drink\": \"What is your favorite drink?\",\n\"repeat\": \"i think i didn't get that, can you repeat it?\",\n\"come\" : \"come with me \",\n\"unreached\": \"I am sorry, I couldn't understand your command, pleasy try again \",\n\"sorry\" : \"I am sorry, I think i didn't get that\",\n\"assign\": \"Can you please sit on the chair in front of me?\"\n}\n</code></pre> they are used as: <pre><code>self.say(promts[\"start\"])\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#calls","title":"Calls","text":"<p>Calls are for GPT-3 API, they are simply a dictionary with instructions to get the entitie of a text <pre><code>calls = {\n\"get_name\": \"if there is nos name return ''get me the name of the person on the next message: \",\n\"get_drink\": \"get me the name of the beverage on the next message: \",\n\"describe\": \"describe a person with the next attributes: \",\n\"confirm\" : \"tell me onlyTrue or False, the next message is a general confirmation, like ues, OK, got it:\",\n\"reached\": \"tell me only True or False, the next message is a confirmation that you reached a place:\",\n\n\"get_loc\": \"the valid locations are living room, dining room, kitchen, bedroom. Give me only the location on this message: \",\n\"get_obj\": \"the valid objects are coke, apple, mug, soap, banana, pitcher. Now give me only the object on this message: \",\n\"get_per\": \" the valid names are Jamie, Morgan, Micheal, Jordam, Taylor, Tracy, Robin, Alex. Now give me only the name on this mesage: \"\n}\n</code></pre></p> <p>commonly used as: <pre><code>intent = self.parser(calls[\"describe\"] + \" \" + str(self.persons[0].name) + \" \" + str(self.persons[0].race) + \" \" + str(self.persons[0].age))\nself.say(intent)\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#configuration","title":"Configuration","text":"<p>As the system requires hardware input and output you need to be aware of some details.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#microphone","title":"Microphone","text":"<p>Select the correct microphone in your system, the easiest way is by graphical interface, but you can also do it by command line. </p> <p><pre><code>arecord -l\n</code></pre> You will get something like this: <pre><code>**** List of CAPTURE Hardware Devices ****\ncard 0: PCH [HDA Intel PCH], device 0: ALC295 Analog [ALC295 Analog]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\n</code></pre> Then you plug the microphone and run the command again: <pre><code>**** List of CAPTURE Hardware Devices ****\ncard 0: PCH [HDA Intel PCH], device 0: ALC295 Analog [ALC295 Analog]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\ncard 2: Device [USB PnP Sound Device], device 0: USB Audio [USB Audio]\n  Subdevices: 0/1\n  Subdevice #0: subdevice #0\n</code></pre> so you can see that the microphone is in the card 2, device 0. So you can set the environment variable: <pre><code>export ALSA_INPUT_DEVICE=hw:2,0\n</code></pre> That should be enough to make the microphone work, but you can record a test to check it: <pre><code>arecord -f cd -Dhw:2,0 -d 10 test.wav\n</code></pre> You can play the test: <pre><code>aplay test.wav\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/#speaker","title":"Speaker","text":"<p>You have two options to configure the speaker, you can: </p> <ul> <li>Use a bluetooth speaker and run the say node in your computer, so you can use the bluetooth speaker with your computer.</li> <li>Use the speaker connected to a plug in the robot and run the say node in the robot. You need to configure the output interface in the jetson, the easiest way is by graphical interface, but you can also do it by command line. </li> </ul> <p>You should do the same as with the microphone, but with the output device: <pre><code>aplay -l\n</code></pre> You will get something like this: <pre><code>**** List of PLAYBACK Hardware Devices ****\ncard 0: PCH [HDA Intel PCH], device 0: ALC295 Analog [ALC295 Analog]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\ncard 1: Device [USB PnP Sound Device], device 0: USB Audio [USB Audio]\n    Subdevices: 1/1\n    Subdevice #0: subdevice #0\n    ```\nThen you plug the speaker and run the command again:\n```bash\n**** List of PLAYBACK Hardware Devices ****\ncard 0: PCH [HDA Intel PCH], device 0: ALC295 Analog [ALC295 Analog]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\ncard 1: Device [USB PnP Sound Device], device 0: USB Audio [USB Audio]\n    Subdevices: 1/1\n    Subdevice #0: subdevice #0\ncard 2: Device_1 [USB PnP Sound Device], device 0: USB Audio [USB Audio]\n    Subdevices: 1/1\n    Subdevice #0: subdevice #0\n</code></pre> so you can see that the speaker is in the card 2, device 0. So you can set the environment variable: <pre><code>export ALSA_OUTPUT_DEVICE=hw:2,0\n</code></pre> That should be enough to make the speaker work, but you can play a test to check it: <pre><code>aplay -test.wav\n</code></pre></p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/GPT-3%20API/","title":"GPT-3 API","text":"<p>We use OpenAI's GPT-3 API to get the entities of the text, to use the api in a fast way we use the playground that OpenAI provides, you can find it here. There you can ask for the commands to check if your request is correct, and then you can use the code that they provide to make the request in your dictionary.</p> <p>Tha base code for an the used parameters is the following:</p> <p><pre><code>def __init__(self):\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    # GPT API\n    self.GPT_temperature=0.7\n    self.GPT_top_p=1\n    self.GPT_frequency_penalty=0    \n    self.GPT_presence_penalty=0\n</code></pre> Remember you should put your API Key as a enviroment variable</p> <p>And a call should look like this: </p> <pre><code>intent = self.callGPT((command + \" \" + self.inputText))\n\ndef callGPT(self, pr, t_max=256):\n        response = openai.Completion.create(\n            model=self.GPT_model,\n            prompt=pr,\n            temperature=self.GPT_temperature,\n            max_tokens=t_max,\n            top_p=1,\n            frequency_penalty=self.GPT_frequency_penalty,\n            presence_penalty=self.GPT_presence_penalty, \n        )\n        return response.choices[0].text\n</code></pre> <p>t_max is the maximium of tokens you can use in the call, if the prompt is too long it will return an error, so you can try to raise the t_max value.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/GPT-3%20API/#entities-extraction","title":"Entities extraction","text":"<p>When we wnt to extract the entities from a text we use the dictionary of calls that we have previously tested in the playground, some examples of calls are:</p> <p><pre><code>calls = {\n\"get_name\": \"if there is nos name return ''get me the name of the person on the next message: \",\n\"get_drink\": \"get me the name of the beverage on the next message: \",\n\"describe\": \"describe a person with the next attributes: \",\n\"confirm\" : \"tell me onlyTrue or False, the next message is a general confirmation, like ues, OK, got it:\",\n\"reached\": \"tell me only True or False, the next message is a confirmation that you reached a place:\",\n\n\"get_loc\": \"the valid locations are living room, dining room, kitchen, bedroom. Give me only the location on this message: \",\n\"get_obj\": \"the valid objects are coke, apple, mug, soap, banana, pitcher. Now give me only the object on this message: \",\n\"get_per\": \"the valid names are Jamie, Morgan, Micheal, Jordam, Taylor, Tracy, Robin, Alex. Now give me only the name on this mesage: \"\n}\n</code></pre> You need to be very specific with the calls, because if you are not, the response will not be the expected and be sure that returns only the entity that you want, because as a completition model it will try to complete the text with more information.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/GPT-3%20API/#openai-tokens","title":"OpenAI Tokens","text":"<p>The tokens are the way that OpenAI uses to charge the API, you can check the tokens that you have used in the billing page. The tokens are charged by the number of tokens that you use in the call, remember you have a free trial of 30000 tokens, so you can use it to test the API. 18 dollars and that should be more than enough for testing the API. Be carefull with the expiration date of trial.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/GPT-3%20API/#previous-test","title":"Previous test","text":"<p>Because GPT-3 API is a powerfull language model we tried to give most of the control to GPT-3 API, but we found that it is not the best way to use it, because as a language model we specified a input and an output format but it doesnt give a stable response and dont follow the rules that you gave him, specially when involves state machines and the response ould depend on the previous state. So we decided to give more control to the code and use GPT-3 API only to get the entities of the text.</p>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/speech_to_text/","title":"Speech To text","text":"<p>The speech to text area is composed by the following ROS nodes:</p> <ol> <li> <p>AudioCapturer</p> <p>devices/AudioCapturer [python]: It is a node that captures the audio using PyAudio and publishes it to the topic rawAudioChunk.</p> </li> <li> <p>GetUsefulAudio</p> <p>devices/UsefulAudio [python]: A node that takes the chunks of audio and, using webrtcvad, checks for a voice, cut the audio and publishes it to the topic UsefulAudio. Webrtcvad approach was made as an alternative that don\u00b4t remove silence but obtains the pieces of audio when someone speaks perfectly, it has a very good performance.</p> </li> <li> <p>Engine Selector </p> <p>action_selectors/hear [python]: This node receives the requests of STT. It checks if there is an internet connection, to know whether to call the offline or online engine; this can be overridden with FORCE_ENGINE parameter. * Online engine: it is in AzureSpeechToText node. For that, this node processes the audio of UsefulAudio do a resample of 16KHz and publishes it in a new topic called UsefulAudioAzure to relay it to that node.  * Offline engine: it is in DeepSpeech node. For that, this node redirect the audio of UsefulAudio to a new topic called UsefulAudioDeepSpeech to relay it to that node.</p> </li> <li> <p>Speech to text Engines     Both are nodes that receive the audio and convert it to text. The difference is that one is online and the other is offline. </p> <p>action_selectors/AzureSpeechToText [c++]: A node that takes the audio published in the topic UsefulAudioAzure and send it to the Azure SpeechToText API, receives the text and publishes it to the topic RawInput.</p> <p>action_selectors/DeepSpeech [python]: A node that takes the audio published in the topic UsefulAudioDeepSpeech and calls DeepSpeech2, converts it to text, and publishes it to the topic RawInput.</p> </li> </ol>"},{"location":"home/2022-Jun%202023/Human%20Robot%20Interaction/speech/text_to_speech/","title":"Text To Speech","text":"<p>Consists of 1 component that is a ROS node with topics.</p> <ol> <li> <p>Say</p> <p>devices/say [python]: It is a node that say through the speakers what is published under robot_text topic. It has a topic to notify another nodes that the robot is talking  inputAudioActive. It uses Google gTTS engine as an online alternative or pyttsx3 as an offline alternative.</p> </li> </ol>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/","title":"Overview","text":"<p>The area of integration and networks is concerned about how all the modules are connected, including hardware and software. </p> <p>The area is in charge of do the state machines of the tasks for Robocup@Home, remember to read frecuently the Robocup@Home Rulebook to be aware of the rules of the competition.</p> <p>In the following sections, we will discuss the following topics:</p> <ul> <li>Jetson Xavier AGX</li> <li>Jetson Nano</li> <li>Network</li> </ul> <p>Integration is a key part of the project, since it is the way to connect all the modules and make them work together. You must have an idea of how the modules are connected and how they communicate with each other. And also how all the modules are working.</p> <p>Also you need to be aware of hardware issues and when you need to change some hardware configuration, for example, if you need more jetson nano devices, in order to have more local processing power.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/#considerations-of-all-the-modules-in-the-network","title":"Considerations of all the modules in the network","text":""},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/#navigation","title":"Navigation","text":"<p>The navigation module is based on a ROS package called ROS Navigation Stack, it is a 2D navigation stack that takes in information from odometry, sensor streams, and a goal pose and outputs safe velocity commands that are sent to a mobile base. Regularly you have a move_base node that is the one that is in charge of the navigation, it takes the goal pose and the odometry information and it sends the velocity commands to the mobile base. </p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/#human-robot-interaction","title":"Human Robot Interaction","text":"<p>The natural interaction with the user is something that you need to consider in the integration, you need to know how the user is going to interact with the robot, and how the robot is going to respond to the user. Also what the robot will do if he doesn't understand the user.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/#speech","title":"Speech","text":"<p>You need to fully understand the calls of the speech module, you can check specific detalis in the Speech Module. Remember all the control topics and be aware of any misspelling thats why is recommended to have a confirmation call for every command given by the user.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/#human-analysis","title":"Human Analysis","text":"<p>The detection or the analysis of the human should be storaged and tagged correctly, the natural implementation of the HRI area should be consider when integrating the system</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/#manipulation","title":"Manipulation","text":"<p>Remember to always check the hardware configurations for any manipulation task and remember that the area of manipulation is in charge of the manipulation of the objects, remember the safety of the user and include routines for different cases from a the object falling or the user not getting the object.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/#vision","title":"Vision","text":"<p>With vision you can check different objects of the environment, and integrate them to any behavior, just rememberthe latency of the vision module and consider the control topics of this module.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Nano/","title":"Jetson Nano Setup","text":"<p>With jetson nano we use the Ubuntu 20.04 image, you can download it from here. You need to flash directly the image to the sd card, you can use balenaEtcher to do it.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Nano/#considerations","title":"Considerations","text":"<ul> <li>You need to have a 32GB sd card and the fixed image is of this size, to get more space you only need to expand the partition with gparted or any other tool.</li> <li>Teamviewer is already installed in the image, but the repository is not updated, so you can errase it in order to get a good output from apt update.</li> <li>Cuda is already installed in the image, you can check it with the following command:</li> </ul> <p><pre><code>nvcc --version\n</code></pre> if is not detected remember to fix the path with the following command:</p> <p><pre><code>export PATH=/usr/local/cuda-&lt;Version&gt;/bin${PATH:+:${PATH}}\n</code></pre> - If you want to intall the Zed SDK you need to check the cuda version and install the correct version of the SDK - If you want to install ROS noetic you can follow the instructions here</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Nano/#hardware-considerations","title":"Hardware considerations","text":"<ul> <li>You need to have a 5V 4A power supply, but you can also use 2 power supplies of 5V 3A. with to the corresponding pins in the board. </li> <li>Fan is not included in the board, you can use a 5V fan and connect it to the 5V and GND pins in the board or you can use a generic 12V fan of 2 inches and connect it to the an external power supply.</li> <li>You can use a wifi dongle to connect the board to the internet, some of them are already compatible with the board, but if is not detected you should check the model and install the drivers for linux. Some of them are not compatible with the board, so you should check the compatibility before buying one.</li> <li>Sometimes usb ports are not working, consider this ports or change the jetson nano board.</li> </ul>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Xavier%20Agx/","title":"Jetson AGX Xavier","text":"<p>To flash the Jetson AGX Xavier, you need to download the JetPack SDK from Nvidia. The JetPack SDK is a collection of software tools that enables the development of AI applications for Jetson. It includes the BSP (Boot Software Package), Linux kernel, NVIDIA CUDA, and TensorRT libraries for deep learning, computer vision, GPU computing, multimedia processing, and much more.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Xavier%20Agx/#flashing-the-jetson-agx-xavier","title":"Flashing the Jetson AGX Xavier","text":"<ol> <li>Download the JetPack SDK from Nvidia. You will need to create an account to download the SDK in Nvidia Developer.</li> <li>Put the jetson AGX Xavier on recovery mode. There is a recovery button on the board, which is in the middle of three buttons. Hold the recovery button and then power it up, which will enter the Force Recovery Mode</li> <li>You should connect the Jetson AGX Xavier to your computer with a USB to USB-C cable. The USB-C port is the one that is closer to the power button. It should detect the board</li> <li>You Can select the components you want to install. We recommend to install all of them.</li> <li>Then you should flash the board (if you are using SSD storage you should change the Storage Device).</li> <li>When you get to install additional components, you will get a promt, at that moment the board has already a initial OS, if you get an error in the connection change to another method and check with a display the real IP of the board.</li> </ol>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Xavier%20Agx/#setting-up-the-jetson-agx-xavier","title":"Setting up the Jetson AGX Xavier","text":"<p>At this moment you should have an ubuntu 20.04 OS in the Jetson AGX Xavier. You can connect the board to a display and a keyboard or you can also connect it to the internet with an ethernet cable or with a wifi dongle.</p> <p>Some basic instructions to set up the board:</p> <ul> <li>ROS noetic installation</li> <li>Zed SDK installation</li> </ul>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Xavier%20Agx/#misc","title":"Misc","text":"<p>There are some issues working with the Jetson AGX Xavier, here are some of them:</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Xavier%20Agx/#visual-studio-code","title":"Visual Studio Code","text":"<p>To run visual studio code you should get an ARM installer, if you cant launch it try with the following command:</p> <pre><code>code --no-sandbox\n</code></pre>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Xavier%20Agx/#cuda","title":"CUDA","text":"<p>Check if cuda is correctly installed with the following command:</p> <p><pre><code>nvcc --version\n</code></pre> if not, you can check if you are only missing the links to cuda with the following command:</p> <p><pre><code>export PATH=/usr/local/cuda-12.0/bin${PATH:+:${PATH}}\n</code></pre> Remember to change the cuda version if you are using another one.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Jetson%20Xavier%20Agx/#zed-sdk","title":"Zed SDK","text":"<p>Check which version of Zed SDK you are installing acording to your cuda version.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/","title":"Network","text":"<p>The comunication between the different devices is done with ROS network. You should set a ROS master in one of the devices and the rest of the devices should connect to it.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#master-machine","title":"Master machine","text":"<p>The master machine is the one that has the ROS master. It can be any of the devices, but we recommend to use the Jetson Xavier AGX so everyone can use the same ROS master. before starting the ROS master you should set up the ROS network with the following commands:</p> <p><pre><code>export ROS_MASTER_URI=http://&lt;ROS_MASTER_IP&gt;:11311\nexport ROS_IP=&lt;ROS_MASTER_IP&gt;\n</code></pre> We recommend to add this commands to the .bashrc file so you dont have to type them every time you open a new terminal. </p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#slave-machines","title":"Slave machines","text":"<p>The slave machines are the ones that connect to the ROS master. Remember to also set up your ip of the local device in every terminal</p> <pre><code>```bash\nexport ROS_MASTER_URI=http://&lt;ROS_MASTER_IP&gt;:11311\nexport ROS_IP=&lt;ROS_SLAVE_DEVICE_IP&gt;\n```\n</code></pre> <p>We recommend to add this commands to the .bashrc file so you dont have to type them every time you open a new terminal. </p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#misc","title":"Misc","text":"<p>In the DashGo Robot there is a modem that creates a wifi network, you can connect to it, also everything that is in the robot is connected with ethernet cables to a switch and then to the modem. You can connect to the switch with an ethernet to have a better connection. but you can also connect to the modem wifi network.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#devices","title":"Devices","text":""},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#ssh-connection","title":"SSH Connection","text":"<p>You can connect to the devices with ssh, you can use the following command:</p> <p><pre><code>ssh &lt;user&gt;@&lt;ip&gt;\n</code></pre> The user for Jetson Xavier AGX is nvidia and the default password is nvidia The user for Jetson Nano is jetson and the default password is jetson</p> <p>We recommend to use visual studio code to connect to the devices, you can install the Remote Development Extension Pack and then connect to the devices with the following command:</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#jetson-xavier-nano","title":"Jetson Xavier nano","text":"<p>For jetson nano you can connect with a micro usb cable, it should be detected as ethernet connection, you can connect to it with the following command:</p> <p><pre><code>ssh jetson@192.168.55.1\n</code></pre> The default ip for jetson nano is 192.168.55.1 when a micro usb cable is connected</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#display","title":"Display","text":"<p>Remember you have a small display to do some basic things in the jetson xaiver agx, you can also connect a keyboard and a mouse to it to make fast configuration changes like audio, wifi, etc.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#troubleshooting","title":"Troubleshooting","text":""},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#not-listening-topics","title":"Not listening topics","text":"<p>If you are having problems with the ROS network, you can check if the ROS master is listening the topics with the following command:</p> <p><pre><code>rostopic list\n</code></pre> Sometimes it will show you the topics but it will not be listening them, you can check if the ROS master is listening the topics with the following command:</p> <pre><code>rostopic info &lt;topic_name&gt;\n</code></pre> <p>If the topic is not listening, you can try to restart the ROS master with the following command: set again the ROS networ, master and your device ip and then restart the ROS master</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#unable-to-communicate-with-master","title":"Unable to communicate with master","text":"<p>Be sure that you have set up the ROS network correctly, and you are running the ROS master in the master machine. If you are still having problems, you can try to restart the ROS master</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#nothing-appears-in-terminal","title":"Nothing appears in terminal","text":"<p>When you run something and it's not properly set check if you are connected to the same network, you can check you ip with the following command:</p> <pre><code>ifconfig\n</code></pre>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#multiple-wifi-near","title":"Multiple WiFi near","text":"<p>When you have multiple WiFi networks near, you can have problems with network, you should check the best channel for your network and set it up in the modem. If needed you can also connect to the switch with an ethernet cable to have a better connection.</p>"},{"location":"home/2022-Jun%202023/Integration%20and%20Networks/Network/#slow-data-transfer","title":"Slow data transfer","text":"<p>In many cases you should transfer data between devices, always be aware of the best way to do it, images, pointclouds and other big data can get you slow frecuencies in topics, you should try to keep all the Real Time processes in the jetson xavier agx and the jetson nano, and the other devices should only be used as debug devices or really large processing devices.</p>"},{"location":"home/2022-Jun%202023/Mechanics/DashGO%20x%20ARM/Design/","title":"Dash Go + xARM","text":""},{"location":"home/2022-Jun%202023/Mechanics/RBGS/Base/","title":"Base Omnidireccional","text":""},{"location":"home/2022-Jun%202023/Mechanics/RBGS/Base/#rediseno-2022-2023","title":"Rediseno 2022 -&gt; 2023","text":"<p>imagenes</p>"},{"location":"home/Aug%202023-Present/","title":"Developments from 2023-2024","text":"<p>This year development is focused around expanding and updating the software areas of our current hardware setup, with a new project based on the development of a custom omnidirectional mobile base.</p>"},{"location":"home/Aug%202023-Present/#electronics-and-control","title":"Electronics and Control","text":"<ul> <li>Started development of PCBs for the omnidirectional mobile base.</li> <li>Standardized the electronics of the main robot for full 24V operation.</li> </ul>"},{"location":"home/Aug%202023-Present/#mechanics","title":"Mechanics","text":"<ul> <li>Started development of the omnidirectional mobile base, with an octagonal shape and 4 omnidirectional wheels.</li> <li>Designed and started manufacturing of several new grippers.</li> <li>Redesigned the electronics protections for the Dashgo and Xarm6.</li> </ul>"},{"location":"home/Aug%202023-Present/#human-robot-interaction","title":"Human Robot Interaction","text":""},{"location":"home/Aug%202023-Present/#speech-recognition","title":"Speech Recognition","text":"<ul> <li>Developed a ROS node for speech recognition using Whisper, replacing our online solution from Azure.</li> </ul>"},{"location":"home/Aug%202023-Present/#action-interpretation","title":"Action Interpretation","text":"<ul> <li>Developed custom fine tuned GPT-3.5 model for action interpretation, with emphasis on a more flexible and human-like interpretation of voice commands.</li> </ul>"},{"location":"home/Aug%202023-Present/#robot-interface","title":"Robot Interface","text":"<ul> <li>Developed a ROS Node for continuous arm movement to follow the user, enhancing the robot's interaction capabilities.</li> <li>Installed and started development on an on-robot tablet for user interaction, including RVIZ and camera visualization and a custom interface for the robot's services.</li> </ul>"},{"location":"home/Aug%202023-Present/#integration-and-networks","title":"Integration and Networks","text":""},{"location":"home/Aug%202023-Present/#ros","title":"ROS","text":"<ul> <li>Replaced a previous web socket solution for communicating two roscores with a multimaster solution, allowing for more robust and faster communication between the robot software modules.</li> <li>Started development of ROS2 migration.</li> </ul>"},{"location":"home/Aug%202023-Present/#computer-vision","title":"Computer Vision","text":""},{"location":"home/Aug%202023-Present/#human-recognition","title":"Human Recognition","text":"<ul> <li>Replaced DeepFace for face_recognition from dlib, allowing for faster and more accurate face recognition.</li> <li>Developed a custom human attribute recognition using the PETA dataset.</li> </ul>"},{"location":"home/Aug%202023-Present/#object-detection","title":"Object Detection","text":"<ul> <li>Migrated to Yolov8 for object detection.</li> <li>Increased adaptability and ease of use of the automated dataset generation method.</li> <li>Started development of a custom dataset generation tool based on 3D scanning.</li> </ul>"},{"location":"home/Aug%202023-Present/#navigation","title":"Navigation","text":""},{"location":"home/Aug%202023-Present/#dashgo-b1-mobile-base","title":"Dashgo B1 mobile base","text":"<ul> <li>Developed a node for fusing LIDAR and on-arm depth camera for better obstacle avoidance.</li> <li>Started work on a person follower node integrating SLAM for navigation on unknown environments.</li> </ul>"},{"location":"home/Aug%202023-Present/#omnidirectional-mobile-base","title":"Omnidirectional Mobile Base","text":"<ul> <li>Started development of the omnidirectional mobile base, with a custom ROS node for control and odometry.</li> <li>Integrated Hector SLAM for mapping and localization.</li> </ul>"},{"location":"home/Aug%202023-Present/#manipulation","title":"Manipulation","text":""},{"location":"home/Aug%202023-Present/#planning-and-hardware","title":"Planning and Hardware","text":"<ul> <li>Replaced Xarm5 for a Xarm6, as well as designed new custom grippers.</li> <li>Migrated the planning and arm control computation to the Jetson AGX Xavier, for stable compute times and a more reliable ethernet-based communication with the arm.</li> <li>Started a migration from MoveIt planning to faster alternatives.</li> </ul>"},{"location":"home/Aug%202023-Present/#pick-and-place","title":"Pick and Place","text":"<ul> <li>Increased accuracy and stability of the pick process.</li> <li>Developed two place methods, one with a graph based approach and another using clustering for identifying possible placing areas.</li> </ul>"},{"location":"home/Aug%202023-Present/Team%20Members/","title":"Team Members 2023-2024","text":""},{"location":"home/Aug%202023-Present/Team%20Members/#electronics-and-control","title":"Electronics and Control","text":"<ul> <li>David V\u00e1zquez</li> <li>\u00c1ngel Cervantes</li> <li>Yair Reyes</li> </ul>"},{"location":"home/Aug%202023-Present/Team%20Members/#mechanics","title":"Mechanics","text":"<ul> <li>Alejandro Guerrero</li> <li>Leonardo S\u00e1nchez</li> <li>Jes\u00fas de Anda</li> <li>Jordan Palafox</li> <li>Diego L\u00f3pez</li> <li>Gustavo G\u00e1mez</li> </ul>"},{"location":"home/Aug%202023-Present/Team%20Members/#integration-and-networks","title":"Integration and Networks","text":"<ul> <li>Iv\u00e1n Romero</li> <li>Ad\u00e1n Flores</li> <li>Emiliano Flores</li> </ul>"},{"location":"home/Aug%202023-Present/Team%20Members/#navigation","title":"Navigation","text":"<ul> <li>Diego Hern\u00e1ndez</li> <li>Oscar Arreola</li> <li>Alexis Chapa</li> </ul>"},{"location":"home/Aug%202023-Present/Team%20Members/#manipulation","title":"Manipulation","text":"<ul> <li>Emiliano Flores</li> <li>Ad\u00e1n Flores</li> <li>Alexis Chapa</li> </ul>"},{"location":"home/Aug%202023-Present/Team%20Members/#computer-vision","title":"Computer Vision","text":"<ul> <li>Iv\u00e1n Romero</li> <li>Emiliano Flores</li> <li>Jonatan de la Rosa</li> <li>Alejandra Coeto</li> <li>Jos\u00e9 Benvenuto</li> </ul>"},{"location":"home/Aug%202023-Present/Team%20Members/#human-robot-interaction","title":"Human Robot Interaction","text":"<ul> <li>Iv\u00e1n Romero</li> <li>Marina Villanueva</li> <li>Oscar Arreola</li> <li>Alejandra Coeto</li> <li>Francisco Salas</li> </ul>"},{"location":"home/Aug%202023-Present/Computer%20Vision/","title":"Computer Vision","text":""},{"location":"home/Aug%202023-Present/Computer%20Vision/#human-recognition","title":"Human Recognition","text":"<ul> <li>Replaced DeepFace for face_recognition from dlib, allowing for faster and more accurate face recognition.</li> <li>Developed a custom human attribute recognition using the PETA dataset.</li> </ul>"},{"location":"home/Aug%202023-Present/Computer%20Vision/#object-detection","title":"Object Detection","text":"<ul> <li>Migrated to Yolov8 for object detection.</li> <li>Increased adaptability and ease of use of the automated dataset generation method.</li> <li>Started development of a custom dataset generation tool based on 3D scanning.</li> </ul>"},{"location":"home/Aug%202023-Present/Computer%20Vision/Human%20Recognition/Face%20detection/","title":"Face detection and recognition","text":"<p>In order to improve efficiency of face detection and recognition, several models were tested, given that past implementations of <code>DeepFace</code> were unstable. Finally, <code>face_recognition</code> from dlib was selected as it allowed for faster and more accurate results.</p>"},{"location":"home/Aug%202023-Present/Computer%20Vision/Human%20Recognition/Face%20detection/#face-detection","title":"Face detection","text":"<p>Before the detection process, previous recognized faces are first loaded from a folder with known people images named with a unique id, thus obtaining and adding each face encoding to a list of known people. Next, each frame from the ZED is processed, finding the face locations and encodings of each face in the frame. In addition, in order to reduce the comparisons during the recognition phase, faces are tracked according to their position and a determined threshold. This way faces that remain in almost the same position as the previous frame are not processed again.</p>"},{"location":"home/Aug%202023-Present/Computer%20Vision/Human%20Recognition/Face%20detection/#face-recognition","title":"Face recognition","text":"<p>For the recognition process, each encoding and location from the frame is compared to the known faces array to determine if there is a match and obtain the corresponding id. If there are no matches, then the face is loaded to the folder of known people and assinged a new id, which is also stored in a json file, which also contains basic characteristics of a person.</p>"},{"location":"home/Aug%202023-Present/Computer%20Vision/Human%20Recognition/Face%20detection/#ros-integration","title":"ROS integration","text":"<p>The detection and recognition node is subscribed to the ZED camera topic, and currently publishes two topics:  - Detection results: which contains the bounding boxes and IDs of the detected faces as a custom list message. - Move topic: which contains the position of the largest detected face in the frame, allowing the robot to follow the person.</p>"},{"location":"home/Aug%202023-Present/Electronics%20and%20Control/","title":"Index","text":""},{"location":"home/Aug%202023-Present/Electronics%20and%20Control/#electronics-and-control","title":"Electronics and Control","text":"<ul> <li>Started development of PCBs for the omnidirectional mobile base.</li> <li>Standardized the electronics of the main robot for full 24V operation.</li> </ul>"},{"location":"home/Aug%202023-Present/Electronics%20and%20Control/#electronics-and-control-development","title":"Electronics and Control development","text":"<p>In order to achieve the full control of an omnidirectional base it was necessary to develop different PCBs and electronics to ensure that the base's movement, responses, and communication protocols met the standards and challenges set by the @Home competition</p> <p>The most important part was the control implementation on the base's movement since it is intended to use a set of 4 omnidirectional wheels instead of mecanum wheels, which was the focus on the previous iteration. The initial step was to replace the old kinematics so the new wheel's movement could be supported. Also, a new PID was made so that the robot's sampling time for control purposes could be reduced up to 100ms with the aiming towards avoiding and filtering noise produced from the encoder's reading. Another important point related to the control was the consideration of the additional weight placed upong the robot's structure, so that the PID controller could move precisely the DC motors even in low speed.</p> <p>Another important point about the new holonomic robot development was the need for more powerful motors and batteries, and the design of a new customized PCB for communication purposes and control. In the currnet state of the robot four 12v DC motors with 70Kg-Cm motors are used to move it, but it is planned to change them for more powerful brushless motors such as those used in FIRST competitions or even industrial-sized NEMA 23 or 34 motors with approximately 110kg-Cm or more torque. </p> <p>Finally, in order to ensure the controller PCB compatibility with high power motors and batteries, it is planned to change the drivers of the controller PCB, so they can controll 24v motors and high currents, such as those used in the NEMA motors.</p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/","title":"Human Robot Interaction","text":"<p>HRI aims to create intuitive and natural ways for humans to interact with robots, allowing for seamless integration of robots into various domains such as healthcare, manufacturing, entertainment, and personal assistance.</p> <p>In Roborregos, our work in this field has been divided into three different goals:</p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/#human-physical-analysis","title":"Human physical analysis","text":"<ul> <li>Replaced DeepFace for face_recognition from DLib, allowing for faster and more accurate face recognition.</li> <li>Developed a custom human attribute recognition using the PETA dataset.</li> </ul>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/#speech","title":"Speech","text":"<p>It is divided into 2 subsections:</p> <p>1.  Speech Recognition - Developed a ROS node for speech recognition using Whisper, replacing our online solution from Azure.</p> <p>2. Action Interpretation (NLP, natural language processing) - Developed custom fine tuned GPT-3.5 model for action interpretation, with emphasis on a more flexible and human-like interpretation of voice commands. </p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/#robot-interface","title":"Robot Interface","text":"<ul> <li>Developed a ROS Node for continuous arm movement to follow the user, enhancing the robot's interaction capabilities.</li> <li>Installed and started development on an on-robot tablet for user interaction, including RVIZ and camera visualization and a custom interface for the robot's services.</li> </ul>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Human%20Physical%20Analysis/Face%20Following/","title":"Face following","text":"<p>To allow for a more human-like interaction, the robot was programmed to follow a person's face when it recieves new instructions. This was achieved by using the face detection and recognition node, which publishes the position of the largest face detected in the frame. This way, joints from the arm are adjusted to keep the person's face centered in the camera's field of view.</p> <p> </p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Robot%20Interface/Display/","title":"Display","text":"<p>In order to allow interaction with the robot and receive visual feedback, a diplay was integrated to the robot.</p> <p> </p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Robot%20Interface/Display/#interface","title":"Interface","text":"<p>For full customization, the interface was developed as a Next.js web application, using Typescript, React and TailwindCSS. In addition, to connect to ROS, <code>roslib</code> was used. This is a Javascript library that uses websockets to connect with rosbridge, allowing for the web application to both publish and subscribe to ROS topics.</p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Robot%20Interface/Display/#features","title":"Features","text":"<p>Users can interact with the display through a navigation bar that allows access to different sections:</p> <ul> <li> <p>Commands: Taking advantage of the main engine, this section shows basic commands for the robot to follow, these being: Go, Grab, Put, Find, Introduce and Stop, each having further options or locations accordingly in order to publish a topic with the desired command.</p> </li> <li> <p>Camera: Shows the Zed camera feed, with the option to visualiza human recognition or object detection frames.</p> </li> <li> <p>Navigation: Still in development, this section allows the user to visualize the the navigation status through RVIZ web.</p> </li> <li> <p>Status: Shows basic information about the robot, such as current task or debugging messages.</p> </li> </ul>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Action%20Interpretation/","title":"NATURAL LANGUAGE PROCESSING (NLP)","text":"<p>Natural language processing (NLP) involves employing machine learning techniques to enable computers to understand, process, and interpret human language effectively. This technology plays a crucial role in action interpretation, bridging the gaps presented by dialects, slang terms, and grammatical nuances commonly encountered in everyday communication. Natural language processing (NLP) NLP tasks involve the deconstruction of human text or speech into more manageable components that computer programs can easily interpret. Some of the capabilities can be:</p> <p>- Word-sense disambiguation, where some words may hold different meanings when used in different scenarios. </p> <p>- Named-entity recognition, where it identifies unique names for people, places, events, companies, and other entities and establishes relationships between the different entities within a sentence. For example, in the sentence \u201cJane went on a vacation to France, and she wants to go again\u201d, the NLP software would identify \"Jane\" and \"France\" as special entities. Additionally, it may employ co-reference resolution to determine if different words refer to the same entity. In the provided example, both \"Jane\" and \"she\" refer to the same person.</p> <p>- Sentiment analysis, where the emotion transmitted by textual data can be interpreted. NLP software analyzes text for words or phrases that show dissatisfaction, happiness, doubt, regret, and other hidden emotions.</p> <p>In our case, after converting the audio input into a text string, our objective has been to use NLP to simplify the sentence and conduct an information extraction process. To achieve this, we have leveraged the GPT-3 API and developed a customized model enabling us to organize the input sentence into a common structure to work with. From there, we can analyze the most significant words to determine which type of command it is (manipulation, movement, introducing\u2026) and then act accordingly.</p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Action%20Interpretation/#our-model","title":"OUR MODEL","text":"<p>Using GPT-3 API, we have created a customized model that enables the separation of long sentences into smaller ones separated by commas and following the structure of action + complement. For example:  -   Input: \u201cBring me the Milk from the kitchen\u201d. -   Output: \u201cgo kitchen, bring milk\u201d. In that way, we can analyze the first and the second word separately and find the best possible approach.</p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Action%20Interpretation/#embeddings","title":"EMBEDDINGS","text":"<p>Embeddings are a natural language processing (NLP) technique that converts human language into mathematical vectors based on their inherent characteristics and associated categories. These vectors encapsulate the essence of words, facilitating more efficient language processing by computers. Through the mapping of words (or phrases) to numerical vectors in a high-dimensional space, embeddings ensure that similar words cluster together, thereby capturing their semantic and contextual relationships effectively.</p> <p>As previously discussed, each desired sentence is broken down into an action + complement. Then, we use an embedding model provided by the GPT-3 API (text-embedding-ada-002) to assign a numerical vector to each of these words. Previously, we have constructed a Panda Dataframe with known words directly associated with common actions such as \u201cgo\u201d, \u201cgrab\u201d, \u201cfind\u201d\u2026 . By comparing the embeddings of known words with those of detected words, we can assess their spatial relationship. If they are located close together, meaning their similarity is high upon analysis, we can assume they refer to the same concept. This allows us to proceed with the routine assuming it is that word, enabling the use of different words to express the same meaning. For example, \u201cgo\u201d and \u201cwalk\u201d should be really close spatially, so if we know that \u201cgo\u201d is a movement action, \u201cwalk\u201d will be too. </p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Action%20Interpretation/#pandas-dataframes","title":"PANDAS DATAFRAMES","text":"<p>Pandas is a powerful open-source Python library that provides high-performance data manipulation and analysis tools. Pandas introduces data structures like DataFrames and Series, which allow users to work with labeled and relational data effortlessly. With its rich functionality, Pandas enables users to perform operations such as indexing, slicing, aggregating, and merging data sets efficiently. </p> <p>In the context of Robocup Competition, we are using Pandas DataFrames to organize words related to the competition. That is, we have created several dataframes in which the actions, places, names and items are organized. In each of them, there are different columns with the element, their specfic category (if applicable) and, the most important thing, their embedding conification. For example, -   Dataframe \u201citems\u201d -   Item \u201capple\u201d -   Specific category \u201cfood\u201d -   Embedding: it will be a mathematical vector of 1536 length</p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Action%20Interpretation/#product-similarities","title":"PRODUCT SIMILARITIES","text":"<p>Firstly, a similarity search is conducted with the verb through the embedding process, followed by a repetition of the process for the complement. To make the process optimal and logical, the complement's similarity search is performed based on the detected main action. This means that, depending on the identified main action, the similarity search for the complement will be carried out only in a specific DataFrame: -   For \"go\" or \"put\", the search is conducted in the location DataFrame. -   For \"grab\", the search is performed in the elements DataFrame. -   For \"find\", the search is executed in the names DataFrame. -   For \u201cintroduce\u201d it is not necessary to search anywhere because it is the only action without complement needed.</p> <p>Now, two scenarios may occur: 1. If the complement is a general category, such as \"drinks,\" a list with all possible drinks will be returned. It will then be the responsibility of the vision department to choose the most suitable option. 2. If the complement is specific, like \"milk,\" then there will be no options available so only one thing will be returned. </p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Action%20Interpretation/#future-work","title":"FUTURE WORK","text":"<p>Some enhancements aimed at making the system more automated and robust include the following:</p> <p>Firstly, there's a desire to automate the process of creating DataFrames with known elements. Currently, there are DataFrames created with actions, locations, items and names published in RoboCupAtHome/gpsr_command_generator. However, if a new category is added, the process of integrating it into the system should be faster and simpler.</p> <p>Another aspect to implement is adding contextual meaning within the similarity analysis. For instance, if the user is instructing the robot to move to the kitchen, it's likely that they are seeking for food, not items from the bathroom like toothpaste. By incorporating this context, the list of possibilities can be further narrowed down, ensuring more efficient traversal without the risk of performing nonsensical actions.</p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Human%20Speech%20Processing/","title":"Human Speech Processing","text":"<p>One aspect that enhances the human-robot interaction is the robot\u2019s ability to interpret human speech and generate text which can be processed further down the pipeline. Several approaches can be used to interpret speech and produce the desired outcomes, each involving trade-offs between precision and computational resources used. The approach followed to achieve a robust implementation consisted of dividing the relevant steps among nodes that handle each task. This promotes decoupling, which not only allows for the replacement of a specific node (for potential improvements) without affecting the other ones, but it also facilitates the usage of the produced code in other projects. In addition, it allows for a highly configurable system, which allows us, for instance, to choose between a node with high precision and computational resources and a node with regular precision with reduced system overhead.</p> <p>The main nodes in the pipeline consist of Voice Activity Detection (VAD) and Speech To Text (STT). </p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Human%20Speech%20Processing/#voice-activity-detection","title":"Voice Activity Detection","text":"<p>The VAD module is used to filter the input data given to the STT node. This reduces the overall computational resources used as the VAD requires less resources than the Speech to Text node. To recognize voice activity, Silero VAD, a pre-trained voice activity detector was used. However, the node can also be configured to use webrtcvad, which is slightly faster than Silero VAD but has a reduced precision for our use case. Both options are used to classify audio segments as voiced or unvoiced, which are then collected and sent to the next node in the pipeline if they fit the established parameters of duration and speech probability. </p>"},{"location":"home/Aug%202023-Present/Human%20Robot%20Interaction/Speech/Human%20Speech%20Processing/#speech-to-text","title":"Speech to Text","text":"<p>For the Speech to Text node, Whisper was used: Whisper is a general-purpose speech recognition model, which can perform Multilingual speech recognition, speech translation, and language identification. Whisper can be used with several models that vary in precision and computational resources, meaning that it can be suitable in a diverse set of situations. However, the system can be configured to use Speech Service from Microsoft Azure as in previous years of development, in case there are low computational resources available.</p>"},{"location":"home/Aug%202023-Present/Integration%20and%20Networks/","title":"Integration and Networks","text":""},{"location":"home/Aug%202023-Present/Integration%20and%20Networks/#ros","title":"ROS","text":"<ul> <li>Replaced a previous web socket solution for communicating two roscores with a multimaster solution, allowing for more robust and faster communication between the robot software modules.</li> <li>Started development of ROS2 migration.</li> </ul>"},{"location":"home/Aug%202023-Present/Manipulation/","title":"Manipulation","text":"<p>Dynamic manipulation systems are crucial for advancing robotics because they allow robots to interact with their environments in ways that go far beyond pre-programmed actions. This capability is  particularly important for service robotics, where robots must navigate unstructured and unpredictable environments. Dynamic manipulation helps robots sense changes, modify trajectories, and grasp objects with variations in shape, size, and material. This opens the door to more complex tasks, from assisting with meal preparation to handling delicate objects in healthcare settings. The ability to adapt in real-time also promotes safe and seamless human-robot collaboration.</p> <p>Our physical implementation consists of a 6-DOF Xarm6 robotic arm utilizing MoveIt for trajectory planning. We developed custom picking and placing functionalities, incorporating additional libraries and algorithms, and leveraging feedback from a Zed2 stereo camera:</p> <ul> <li>Planning and Hardware</li> <li>Pick and Place</li> </ul> <p>The developments for this area are in: GitHub repo Manipulation branch</p>"},{"location":"home/Aug%202023-Present/Manipulation/Pick%20and%20Place/","title":"Pick and Place","text":"<p>The process starts with vision information from 2D object detection nodes as well as 3D point cloud information being processed both for octomap generation and object extraction. The robot uses standard OMPL algorithms from <code>MoveIt!</code> for motion planning but the action servers for grasping and placing objects were developed from scratch, specific to the robot capabilities.</p>"},{"location":"home/Aug%202023-Present/Manipulation/Pick%20and%20Place/#picking-objects","title":"Picking objects","text":"<p>Once a requested object is detected from the live image, a <code>RANSAC</code> algorithm is used to to find the table the robot is looking at and cuts it off from the <code>pointcloud</code>, which leaves the object meshes available for identification. This process has proven to be highly consistent in obtaining clean meshes with an adequate speed, and has been the object 3D extraction solution since the AirLab Stacking Challenge.  </p> <p>A Grasping Pose Detector (GPD) model is then used to obtain possible gripper positions to pick the object matched with the 2D detection position, and MoveIt is used to plan the trajectory considering the robot itself and the environment (through a live octomap) to avoid collisions. </p>"},{"location":"home/Aug%202023-Present/Manipulation/Pick%20and%20Place/#placing-objects","title":"Placing objects","text":"<p>Considering the possibilities of non-empty tables where the robot should place objects, two different methods have been developed to build possible placing positions.</p> <p>The first approach is a language-level <code>C++</code> algorithm that finds the best spot for a place position. The development for this solution uses only STL libraries within the language and not any other 3rd party libraries for the processing. To avoid inefficient run times, each real-world 3D point targeted is processed using a hash function to be mapped into sections of 5cm. Initially, a boolean matrix is defined as being true if the robot can reach a spot, and false otherwise. RANSAC is used again to find the biggest table in the robot vision and creates a new boolean matrix with the table limits and the hash function described previously. A new possible placings matrix is generated by comparing and finding the intersection between the robot possible positions and the table points. After finding the table, PCL is used to find the clusters of each object above the table, these slots in the possible placings matrix are removed after mapping the objects points. When all the 3D objects have been processed, the boolean possible placings matrix is complete. A prefix sum area matrix is then computed by following the simple formulae of: Each position in this integer matrix represents the number of slots of 5cm available for placing before it. As a placing near the center of the robot is preferred, a Breadth First Search algorithm is run in this prefix sum area matrix, when it find a position with area of minimum 1.5x the surface area of the object currently held, it returns the center of this area, remapped with the hash function reversed, as a Pose Stamped for the placing position. Then, MoveIt is used for the planning of the trajectory towards the Pose given. When the position is reached, it opens the gripper, placing the object.</p> <p>The second solution aims at solving scenarios with more complex table and object positions and angles relative to the robot, as it considers all points in the pointcloud as they are. First, the point cloud is filtered to only consider the area reachable by the Xarm6, and executes the RANSAC to obtain only the table point cloud, which is sent to a clustering server for processing. It also considers the objects observed in the filtered area and also sends the number of objects present in that area of the table. The server processes the point cloud received as a 2D scatter plot, over which it runs a k-means clustering algorithm, which resulting clusters are used to evaluate the area with the most available space. The number of clusters generated is proportional to the number of objects in the area observed. A factor for multiplication is added so that, for every object, N number of clusters is generated so that different clusters are generated around each object. Currently, 4 clusters per object are analyzed, following that for every object a cluster is generated in each cardinal direction.  </p> <p>After every cluster is generated, the one with the largest area is selected. The area is calculated through generation of a point density histogram for each cluster, which divides the plot on a size proportional grid and uses a minimum pixel density threshold to count for occupied and unoccupied areas of the histogram. The largest cluster obtained then has its centroid calculated and is sent as a 3D pose for object placing.</p>"},{"location":"home/Aug%202023-Present/Manipulation/Planning%20and%20Hardware/","title":"Planning and Hardware","text":"<p>In our latest iteration, we replaced the Xarm5 arm with a Xarm6. This upgrade provides an extra degree of freedom (DOF). Importantly, both arms have public <code>xarm-ros</code> packages for planning with MoveIt.</p> <p>Another significant change was mounting the Zed2 stereo camera directly on the arm's last axis. Previously, its static position at the robot's base frequently obstructed trajectories during the planning process, causing delays. To decrease latency in arm motion, we now run MoveIt on the Jetson Xavier AGX.</p> <p></p>"},{"location":"home/Aug%202023-Present/Mechanics/","title":"Mechanics","text":"<ul> <li>Started development of the omnidirectional mobile base, with an octagonal shape and 4 omnidirectional wheels.</li> <li>Designed and started manufacturing of several new grippers.</li> <li>Redesigned the electronics protections for the Dashgo and Xarm6.</li> </ul>"},{"location":"home/Aug%202023-Present/Navigation/","title":"Navigation","text":""},{"location":"home/Aug%202023-Present/Navigation/#dashgo-b1-mobile-base","title":"Dashgo B1 mobile base","text":""},{"location":"home/Aug%202023-Present/Navigation/#multi-level-obstacle-avoidance","title":"Multi-level obstacle avoidance","text":"<ul> <li>Developed a node for fusing LIDAR and on-arm depth camera for better obstacle avoidance.</li> </ul>"},{"location":"home/Aug%202023-Present/Navigation/#person-following","title":"Person following","text":"<p>For this task, the robot has to find the person to follow, then calculate their position, and finally send it to the navigation server periodically to follow them.</p> <p>First, the robot uses the depth camera to find the person. The depth camera is mounted on the arm of the robot, so the robot has to move the arm to a specific position to be able to search for the person. </p> <p>The robot then employs the Mediapipe pose landmarker model to find the person's landmarks, which are given as pixel coordinates. The pixel coordinates are then converted to 3D coordinates using the depth camera. The robot then transforms the 3D coordinates to the base frame and sends the position to the navigation server.</p> <p>The navigation system utilizes the Hector SLAM package for mapping and localization in unknown environments, as well as obstacle avoidance. Previously the robot used the AMCL package for localization, but it wasn't able to localize the robot accurately in unknown environments. </p> <p>One problem found with the Hector SLAM package is that sometimes the footsteps of the person are detected as obstacles, so the robot stops following the person. A temporary solution was to decrease the speed of the robot to give it more time to mark the footsteps as free cells. Another proposed solution is to use the depth camera to detect the person's footsteps and mark them as free cells manually in the occupancy grid. This solution is still under development.</p>"},{"location":"home/Aug%202023-Present/Navigation/#omnidirectional-mobile-base","title":"Omnidirectional Mobile Base","text":"<ul> <li>Started development of the omnidirectional mobile base, with a custom ROS node for control and odometry.</li> <li>Integrated Hector SLAM for mapping and localization.</li> </ul>"},{"location":"util/markdown/","title":"Getting Started with Markdown","text":"<p>Markdown is a simple markup language that allows you to write using a simple sintax. It's used in many places because of how easy it's to use, understand and read.</p>"},{"location":"util/markdown/#headings","title":"Headings","text":"<p>Headings are created using the <code>#</code> symbol. The more <code>#</code> you use, the smaller the heading will be.</p> <p>Example: <pre><code># Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n</code></pre></p>"},{"location":"util/markdown/#text","title":"Text","text":"<p>Text is written as it is. You can use bold and italic text. You can also use ~~strikethrough~~ text. </p> <p>Example: <pre><code>This is a normal text. You can use **bold** and *italic* text. You can also use ~~strikethrough~~ text. \n</code></pre></p>"},{"location":"util/markdown/#lists","title":"Lists","text":"<p>You can create lists using the <code>-</code> symbol. You can also create numbered lists using the <code>1.</code> symbol.</p> <p>Example: <pre><code>- Item 1\n- Item 2\n    - Item 2.1\n    - Item 2.2\n1. Item 1\n2. Item 2\n    1. Item 2.1\n    2. Item 2.2\n</code></pre> Example output:</p> <ul> <li>Item 1</li> <li>Item 2<ul> <li>Item 2.1</li> <li>Item 2.2</li> </ul> </li> <li>Item 1</li> <li>Item 2<ol> <li>Item 2.1</li> <li>Item 2.2</li> </ol> </li> </ul>"},{"location":"util/markdown/#links","title":"Links","text":"<p>You can create links using the <code>[text](link)</code> sintax.</p> <p>Example: <pre><code>[RoBorregos](\n    https://www.roborregos.com\n)\n</code></pre> Example output: RoBorregos</p>"},{"location":"util/markdown/#images","title":"Images","text":"<p>Similar to links, you can add images using the <code>![alt text](image link)</code> sintax.</p> <p>Example: <pre><code>![RoBorregos Logo](https://github.com/RoBorregos.png)\n</code></pre> Example output: </p>"},{"location":"util/markdown/#code","title":"Code","text":"<p>You can add code using the <code>`</code> symbol. You can also add code blocks using the ``` symbol.</p> <p>Example: <pre><code>`print(\"Hello World\")`\n</code></pre> Example output: <code>print(\"Hello World\")</code></p> <p>Example: <pre><code>    ```python\n    print(\"Hello World\")\n    ```\n</code></pre> Example output: <pre><code>print(\"Hello World\")\n</code></pre></p>"},{"location":"util/markdown/#tables","title":"Tables","text":"<p>You can create tables using the <code>|</code> symbol.</p> <p>Example: <pre><code>| Name | Email | Role |\n| ---- | ----- | ---- |\n| Ivan | [i.wells.ar@gmail.com](mailto:i.wells.ar@gmail.com) | Software Developer, Repo Mantainer and Automatization |\n</code></pre></p> <p>Example output:</p> Name Email Role Ivan i.wells.ar@gmail.com Software Developer, Repo Mantainer and Automatization"},{"location":"util/markdown/#quotes","title":"Quotes","text":"<p>You can create quotes using the <code>&gt;</code> symbol.</p> <p>Example: <pre><code>&gt; This is a quote\n</code></pre> Example output:</p> <p>This is a quote</p>"},{"location":"util/markdown/#horizontal-rule","title":"Horizontal Rule","text":"<p>You can create a horizontal rule using the <code>---</code> symbol.</p> <p>Example: <pre><code>---\n</code></pre> Example output:</p>"},{"location":"util/markdown/#todo-list","title":"ToDo List","text":"<p>You can create a task list using the <code>- [ ]</code> symbol.</p> <p>Example: <pre><code>- [ ] ToDo \n- [x] Done ToDo\n</code></pre> Example output:</p> <ul> <li>[ ] ToDo</li> <li>[x] Done ToDo</li> </ul>"}]}