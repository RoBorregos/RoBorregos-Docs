{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to RoBorregos Documentation","text":"<p>This is the documentation for RoBorregos. Here you can find information about the team, the projects we have worked on, and the tools we use.</p>"},{"location":"#roborregos","title":"RoBorregos","text":"<p>RoBorregos is the Tecnol\u00f3gico de Monterrey's International Robotics Representative Team. We are a team of students passionate about robotics and technology that have several projects in which we participate. To learn more about us, visit our website.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>Projects</li> <li>Util</li> </ul>"},{"location":"#tools","title":"Tools","text":"<ul> <li>ROS</li> </ul>"},{"location":"#development-team","title":"Development team","text":"Name Email Github Role Iv\u00e1n Romero i.wells.ar@gmail.com @IvanRomero03 Software Developer, Repo Mantainer and Automatization Vic @gmail.com @ Software Developer, Repo Mantainer and Automatization Kevin @gmail.com @ PM"},{"location":"RescueMaze/","title":"@RescueMaze","text":"<p>Simulation of a disaster area where the robot has to navigate through the majority of a maze, detect victims through different stimuli (visual images), and evade obstacles. The maze may have multiple floors and the robot must be autonomous.</p>"},{"location":"RescueMaze/#competition","title":"Competition","text":"<p>See the rules for Rescue Maze 2023.</p>"},{"location":"RescueMaze/#sections","title":"Sections","text":"<ul> <li>Jetson Nano</li> </ul>"},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/","title":"Running Project on Jetson nano","text":""},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/#launching-files","title":"Launching files","text":"<ul> <li>Initialize sensors, hector slam, move base, and services:</li> </ul> <pre><code>roslaunch nav_main launch_jetson.launch\n</code></pre> <ul> <li>Run navigation algorithm:</li> </ul> <pre><code>roslaunch exploration main\n</code></pre> <ul> <li>Use rviz from external computer:</li> </ul> <pre><code>export ROS_IP=YOUR_IP\nexport ROS_MASTER_URI=JETSON_IP\n\nrosrun rviz rviz -d $(rospack find robot_description)/rviz/urdf.rviz\n</code></pre>"},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/#connecting-to-jetson-using-ssh","title":"Connecting to Jetson using SSH","text":"<ul> <li>Obtain jetson's IP</li> <li>Perform SSH</li> <li>Introduce password</li> </ul> <pre><code>sudo nmap -sn YOUR_IP/24\nssh username_jetson@JETSON_IP\n</code></pre>"},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/#debug-using-teleop","title":"Debug using teleop","text":"<ul> <li>Make sure ROS_IP and ROS_MASTER_URI are properly set if using another laptop.</li> </ul> <pre><code>rosrun teleop_twist_keyboard teleop_twist_keyboard.py _speed:=0.8 _turn:=2.4 _repeat_rate:=10\n</code></pre>"},{"location":"RescueMaze/Jetson%20Nano/RunningJetson/#add-new-files-to-jetson","title":"Add new files to jetson","text":"<p>If new code was implemented outside of the jetson, the file(s) can be copied using the following command:</p> <pre><code># use -r (recursive flag) for folders\nscp -r SOURCE/ DESTINATION/\n# e.g pass files from laptop to jetson (run command on laptop terminal with ssh connected to jetson)\nscp -r /home/oscar/maze_ws/src/devices/ jetson@IP:/home/jetson/maze_ws/src/\n</code></pre> <p>Also, you may want to consider deleting the files from the jetson first before using scp with the new files:</p> <pre><code># e.g. deleting devices folder before scp\n# in jetson\nrm -rf /home/jetson/maze_ws/src/devices\n</code></pre> <p>Finally, use catkin_make to apply changes.</p> <pre><code>cd ~/maze_ws\ncatkin_make\n</code></pre>"},{"location":"RescueMaze/Jetson%20Nano/USBRules/","title":"USB Port automatization","text":""},{"location":"RescueMaze/Jetson%20Nano/USBRules/#this-file-is-part-of-the-roborregos-rescuemaze-project","title":"This file is part of the RoBorregos RescueMaze project.","text":"<p>Here is an example of how we can se the behaviour of the usb ports.</p> <p></p>"},{"location":"RescueMaze/Jetson%20Nano/USBRules/#udev-rules","title":"Udev Rules","text":"<p>For the USB port automatization, we use udev rules. These rules are located in the <code>rules.d</code> folder. The rules are loaded by the udev daemon when the system starts. The udev daemon monitors the kernel for events and executes the rules when a device is added or removed.</p> <p>To automate the USB ports, we need to create a rule for each port. The rule will be executed when the port is connected to the computer. The rule will execute a script that will set the port to the desired mode.</p> <p>Here is the hard investigation we did to determine the rules:</p> <p></p> <p></p>"},{"location":"home/","title":"@Home","text":"<p>@Home is one of the main competitions for RoBorregos, since it contains a lot of the knowledge that we have acquired throughout the years. It's a complex competitions in multiple levels, which makes it a great challenge for the team.</p>"},{"location":"home/#competition","title":"Competition","text":"<p>The competition consists of a series of tasks that the robot must complete. ...</p>"},{"location":"home/#sections","title":"Sections","text":"<ul> <li>Computer Vision</li> </ul>"},{"location":"home/vision/","title":"Computer Vision","text":"<p>Computer Vision is one of the main areas of development for RoBorregos in the @Home competition. It is a very important area, since it is the one that allows the robot to perceive the environment and interact with it.</p>"},{"location":"home/vision/#sections","title":"Sections","text":"<ul> <li>Object Detection</li> <li>Pose Estimation</li> </ul>"},{"location":"home/vision/pose_estimation/","title":"Pose Estimation with MediaPipe","text":"<p>Pose estimation was implemented using MediaPipe for the RoboCup 2022 @Home Simulation competition. The pose estimation algorithm is based on the MediaPipe Pose solution. </p> <p>It's very simple, acurate and fast. It's also very easy to use, since it's a pre-trained model that can be used directly.</p>"},{"location":"home/vision/pose_estimation/#how-to-use-it","title":"How to use it","text":"<p>First of all, you need to install MediaPipe. You can do it by running the following command:</p> <pre><code>pip install mediapipe\n</code></pre> <p>Then, you can use the following code to get the pose estimation:</p> <pre><code>import mediapipe as mp\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n# Opening the image source to be used\nimage = cv2.imread(\"image.jpg\")\n# Calling the pose detection model\nwith mp_pose.Pose(\nmin_detection_confidence=0.5,\nmin_tracking_confidence=0.5) as pose:\n# Detecting the pose with the image\nposeResult = pose.process(image)\n</code></pre> <p>As a result, you'll have a <code>poseResult</code> array of points. That each point represent a joint of the body, as shown in the following image:</p> <p></p>"},{"location":"home/vision/pose_estimation/#using-pose-estimation-with-webcam","title":"Using pose estimation with webcam","text":"<p>You can also use pose estimation with a webcam to get streamed video. You can use the following code to do it:</p> <pre><code>import mediapipe as mp\nimport cv2\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n# Calling the solution for image drawing from MediaPipe\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n# Opening the webcam\ncap = cv2.VideoCapture(0)\n# Calling the pose detection model\nwith mp_pose.Pose(\nmin_detection_confidence=0.5,\nmin_tracking_confidence=0.5) as pose:\n# Looping through the webcam frames\nwhile cap.isOpened():\n# Reading the webcam frame\nsuccess, image = cap.read()\nif success:\n# Managing the webcam frame\nimage.flags.writeable = False\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n# Detecting the pose with the image\nresults = pose.process(image)\n# Drawing the pose detection results\nimage.flags.writeable = True\nimage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\nmp_drawing.draw_landmarks(\nimage,\nresults.pose_landmarks,\nmp_pose.POSE_CONNECTIONS,\nlandmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\ncv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\nif cv2.waitKey(5) &amp; 0xFF == 27:\nbreak\ncap.release()\n</code></pre> <p>As a result, you'll not only be able to get the pose estimation array. but also the stream with the drawing of the pose estimation.</p> <p>Example:</p> <p></p>"},{"location":"home/vision/pose_estimation/#using-pose-estimation-with-ros","title":"Using pose estimation with ROS","text":"<p>You can receive the image source from a ROS topic. You can use the following code to do it:</p> <pre><code>import mediapipe as mp\nfrom time import sleep\nfrom typing import Tuple\nimport cv2\nimport numpy as np\nimport rospy\nfrom cv_bridge import CvBridge\nfrom sensor_msgs.msg import Image\n# Calling the pose solution from MediaPipe\nmp_pose = mp.solutions.pose\n# Calling the solution for image drawing from MediaPipe\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n# Declaring the CvBridge for image conversion from ROS to OpenCV\nbridge = CvBridge()\n# Declaring the image and its callback for the ROS topic\nimageReceved = None\ndef image_callback(data):\nglobal imageReceved\nimageReceved = data\n# Initializing the ROS node\nrospy.init_node('ImageRecever', anonymous=True)\n# Subscribing to the ROS topic\nimageSub = rospy.Subscriber(\n\"/hsrb/head_center_camera/image_raw\", Image, image_callback)\n# Calling the pose detection model\nwith mp_pose.Pose(\nmin_detection_confidence=0.5,\nmin_tracking_confidence=0.5) as pose:\n# Looping through the image frames\nwhile not rospy.is_shutdown():\nif imageReceved is not None:\n# Converting the ROS image to OpenCV\nimage = bridge.imgmsg_to_cv2(imageReceved, \"rgb8\")\n# Detecting the pose with the image\nimage.flags.writeable = False\nresults = pose.process(image)\n# Drawing the pose detection results\nimage.flags.writeable = True\nimage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\nmp_drawing.draw_landmarks(\nimage,\nresults.pose_landmarks,\nmp_pose.POSE_CONNECTIONS,\nlandmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\ncv2.imshow('MediaPipe Pose', image)\nif cv2.waitKey(5) &amp; 0xFF == 27:\nbreak\nelse:\nprint(\"Image not recived\")\nsleep(1)\n</code></pre> <p>Here is an example of the result:</p> <p></p>"},{"location":"util/markdown/","title":"Getting Started with Markdown","text":"<p>Markdown is a simple markup language that allows you to write using a simple sintax. It's used in many places because of how easy it's to use, understand and read.</p>"},{"location":"util/markdown/#headings","title":"Headings","text":"<p>Headings are created using the <code>#</code> symbol. The more <code>#</code> you use, the smaller the heading will be.</p> <p>Example: <pre><code># Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n</code></pre></p>"},{"location":"util/markdown/#text","title":"Text","text":"<p>Text is written as it is. You can use bold and italic text. You can also use ~~strikethrough~~ text. </p> <p>Example: <pre><code>This is a normal text. You can use **bold** and *italic* text. You can also use ~~strikethrough~~ text. \n</code></pre></p>"},{"location":"util/markdown/#lists","title":"Lists","text":"<p>You can create lists using the <code>-</code> symbol. You can also create numbered lists using the <code>1.</code> symbol.</p> <p>Example: <pre><code>- Item 1\n- Item 2\n    - Item 2.1\n    - Item 2.2\n1. Item 1\n2. Item 2\n    1. Item 2.1\n    2. Item 2.2\n</code></pre> Example output:</p> <ul> <li>Item 1</li> <li>Item 2<ul> <li>Item 2.1</li> <li>Item 2.2</li> </ul> </li> <li>Item 1</li> <li>Item 2<ol> <li>Item 2.1</li> <li>Item 2.2</li> </ol> </li> </ul>"},{"location":"util/markdown/#links","title":"Links","text":"<p>You can create links using the <code>[text](link)</code> sintax.</p> <p>Example: <pre><code>[RoBorregos](\n    https://www.roborregos.com\n)\n</code></pre> Example output: RoBorregos</p>"},{"location":"util/markdown/#images","title":"Images","text":"<p>Similar to links, you can add images using the <code>![alt text](image link)</code> sintax.</p> <p>Example: <pre><code>![RoBorregos Logo](https://github.com/RoBorregos.png)\n</code></pre> Example output: </p>"},{"location":"util/markdown/#code","title":"Code","text":"<p>You can add code using the <code>`</code> symbol. You can also add code blocks using the ``` symbol.</p> <p>Example: <pre><code>`print(\"Hello World\")`\n</code></pre> Example output: <code>print(\"Hello World\")</code></p> <p>Example: <pre><code>    ```python\nprint(\"Hello World\")\n    ```\n</code></pre> Example output: <pre><code>print(\"Hello World\")\n</code></pre></p>"},{"location":"util/markdown/#tables","title":"Tables","text":"<p>You can create tables using the <code>|</code> symbol.</p> <p>Example: <pre><code>| Name | Email | Role |\n| ---- | ----- | ---- |\n| Ivan | [i.wells.ar@gmail.com](mailto:i.wells.ar@gmail.com) | Software Developer, Repo Mantainer and Automatization |\n</code></pre></p> <p>Example output:</p> Name Email Role Ivan i.wells.ar@gmail.com Software Developer, Repo Mantainer and Automatization"},{"location":"util/markdown/#quotes","title":"Quotes","text":"<p>You can create quotes using the <code>&gt;</code> symbol.</p> <p>Example: <pre><code>&gt; This is a quote\n</code></pre> Example output:</p> <p>This is a quote</p>"},{"location":"util/markdown/#horizontal-rule","title":"Horizontal Rule","text":"<p>You can create a horizontal rule using the <code>---</code> symbol.</p> <p>Example: <pre><code>---\n</code></pre> Example output:</p>"},{"location":"util/markdown/#todo-list","title":"ToDo List","text":"<p>You can create a task list using the <code>- [ ]</code> symbol.</p> <p>Example: <pre><code>- [ ] ToDo \n- [x] Done ToDo\n</code></pre> Example output:</p> <ul> <li>[ ] ToDo</li> <li>[x] Done ToDo</li> </ul>"}]}